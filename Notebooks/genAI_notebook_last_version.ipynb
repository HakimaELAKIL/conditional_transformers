{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb39882",
   "metadata": {},
   "source": [
    "DrugGPT Model for molecules generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "SCRIPT COMPLET (A à Z) : TRANSFORMATEUR CONDITIONNEL (Catégoriel) - V2 CORRIGÉ AVEC DRIVE ET PLOTS\n",
    "ET TOUS LES OBJECTIFS AJOUTÉS\n",
    "\n",
    "Objectif : Entraîner un modèle de type GPT à générer des SMILES\n",
    "          en fonction de catégories de propriétés (LogP, MW, HBD, etc.)\n",
    "          avec tous les objectifs spécifiés.\n",
    "\"\"\"\n",
    "\n",
    "# --- PARTIE 1 : IMPORTS ET CONFIGURATION ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc # Garbage collector\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import pour Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"✓ Environnement Google Colab détecté\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"⚠ Environnement local détecté\")\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from rdkit.Chem import Lipinski\n",
    "    from rdkit.Chem import Crippen\n",
    "    from rdkit import rdBase\n",
    "    rdBase.DisableLog('rdApp.error')\n",
    "except ImportError:\n",
    "    print(\"Erreur : RDKit n'est pas installé.\")\n",
    "    print(\"Veuillez l'installer avec : pip install rdkit\")\n",
    "    exit()\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 128\n",
    "\n",
    "MAX_ITERS = 5000 \n",
    "EVAL_INTERVAL = 500\n",
    "EVAL_ITERS = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "N_EMBD = 128\n",
    "N_HEAD = 4\n",
    "N_LAYER = 4\n",
    "DROPOUT = 0.1\n",
    "# Augmenter la dimension des conditions pour inclure tous les objectifs\n",
    "CONDITION_DIM = 10 # LogP_cat, MW_cat, HBD_cat, HBA_cat, RotBonds_cat, AromaticRings, NonAromaticRings, HasFunctionalGroup, R_Value, LipinskiCompliant\n",
    "\n",
    "# Configuration Google Drive\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_PATH = '/content/drive/MyDrive/cond_gpt_model1'\n",
    "    os.makedirs(DRIVE_PATH, exist_ok=True)\n",
    "    print(f\"✓ Dossier Drive créé : {DRIVE_PATH}\")\n",
    "else:\n",
    "    DRIVE_PATH = './local_save'\n",
    "    os.makedirs(DRIVE_PATH, exist_ok=True)\n",
    "    print(f\"✓ Dossier local créé : {DRIVE_PATH}\")\n",
    "\n",
    "# Fichiers\n",
    "DATA_FILE = 's_100_str_+1M_fixed.txt'\n",
    "VOCAB_FILE = os.path.join(DRIVE_PATH, 'vocab_dataset.json')\n",
    "DATA_CACHE_FILE = os.path.join(DRIVE_PATH, 'data_cache_categorical_extended.pt')\n",
    "\n",
    "# Checkpoints\n",
    "CHECKPOINT_DIR = os.path.join(DRIVE_PATH, 'checkpoints')\n",
    "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, 'cond_gpt_categorical_extended.pth')\n",
    "PLOTS_DIR = os.path.join(DRIVE_PATH, 'plots')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration : Périphérique={DEVICE}, Batch={BATCH_SIZE}, Contexte={BLOCK_SIZE}\")\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# --- PARTIE 2 : CONSTRUCTION DU VOCABULAIRE ---\n",
    "\n",
    "print(f\"Construction du vocabulaire à partir de '{DATA_FILE}'...\")\n",
    "\n",
    "# Tokens spéciaux\n",
    "PAD_TOKEN = '<pad>'\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "\n",
    "# Construction du vocabulaire à partir du dataset\n",
    "char_set = set()\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        smiles = line.strip()\n",
    "        char_set.update(list(smiles))\n",
    "\n",
    "# Ajouter les tokens spéciaux\n",
    "special_tokens = [PAD_TOKEN, START_TOKEN, END_TOKEN]\n",
    "vocabulary = special_tokens + sorted(list(char_set))\n",
    "\n",
    "# Dictionnaires pour encoder/décoder\n",
    "stoi = { ch:i for i,ch in enumerate(vocabulary) }\n",
    "itos = { i:ch for i,ch in enumerate(vocabulary) }\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Sauvegarde en JSON\n",
    "with open(VOCAB_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump({'stoi': stoi, 'itos': itos}, f, indent=2)\n",
    "\n",
    "print(f\"Taille vocabulaire : {vocab_size}\")\n",
    "print(\"Exemple de mapping :\", list(stoi.items())[:10])\n",
    "\n",
    "# Définir les fonctions globales encode/decode\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "decode = lambda l: ''.join([itos[i] for i in l if i in itos])\n",
    "\n",
    "# Test du vocabulaire\n",
    "print(\"\\n=== TEST DU VOCABULAIRE ===\")\n",
    "test_smiles = \"CCO\"\n",
    "print(f\"Test encode/decode: {test_smiles}\")\n",
    "encoded = encode(test_smiles)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "decoded = decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Match: {test_smiles == decoded}\")\n",
    "\n",
    "# --- PARTIE 3 : PRÉPARATION DES DONNÉES  ---\n",
    "\n",
    "# Fonction pour assigner une catégorie basée sur les intervalles\n",
    "def get_category(value, bins):\n",
    "    for i, upper_bound in enumerate(bins):\n",
    "        if value <= upper_bound:\n",
    "            return float(i) # Retourne l'indice de la catégorie en float\n",
    "    return float(len(bins))\n",
    "\n",
    "# Fonction pour détecter les groupes fonctionnels spécifiques\n",
    "def has_functional_group(mol):\n",
    "    \"\"\"Vérifie la présence d'au moins un des groupes fonctionnels: -OH, -COOR, -COOH, ou -NH2\"\"\"\n",
    "    smarts_patterns = [\n",
    "        '[OH]',           # -OH\n",
    "        '[#6]C(=O)[O;H0]', # -COOR (ester)\n",
    "        'C(=O)[OH]',      # -COOH\n",
    "        '[NH2]'           # -NH2\n",
    "    ]\n",
    "\n",
    "    for pattern in smarts_patterns:\n",
    "        if mol.HasSubstructMatch(Chem.MolFromSmarts(pattern)):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "# Fonction pour calculer la valeur R \n",
    "def calculate_r_value(mol):\n",
    "    \"\"\"Calcule le R-value comme décrit dans [13]\"\"\"\n",
    "    try:\n",
    "        # Cette métrique peut être définie différemment selon la référence\n",
    "        # Pour cet exemple, nous utilisons un ratio simple\n",
    "        mol_wt = Descriptors.MolWt(mol)\n",
    "        logp = Crippen.MolLogP(mol)\n",
    "\n",
    "        if mol_wt > 0:\n",
    "            # R-value simple basé sur la littérature\n",
    "            r_value = logp / (mol_wt / 100)\n",
    "            return r_value\n",
    "        else:\n",
    "            return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Fonction pour vérifier la compliance Lipinski\n",
    "def check_lipinski_compliance(mol):\n",
    "    \"\"\"Vérifie la règle de Lipinski complète\"\"\"\n",
    "    logp = Crippen.MolLogP(mol)\n",
    "    mol_wt = Descriptors.MolWt(mol)\n",
    "    hbd = Lipinski.NumHDonors(mol)\n",
    "    hba = Lipinski.NumHAcceptors(mol)\n",
    "    rotatable_bonds = Lipinski.NumRotatableBonds(mol)\n",
    "\n",
    "    conditions = [\n",
    "        logp <= 3,\n",
    "        mol_wt <= 480,\n",
    "        hbd <= 3,\n",
    "        hba <= 3,\n",
    "        rotatable_bonds <= 3\n",
    "    ]\n",
    "\n",
    "    return 1.0 if all(conditions) else 0.0\n",
    "\n",
    "# Définir les bornes supérieures des intervalles\n",
    "LOGP_BINS = [0.0, 3.0, 5.0] # <=0(0), <=3(1), <=5(2), >5(3)\n",
    "MW_BINS = [250.0, 480.0, 650.0] # <=250(0), <=480(1), <=650(2), >650(3)\n",
    "HBD_BINS = [0.0, 1.0, 2.0, 3.0] # =0(0), =1(1), =2(2), =3(3), >3(4)\n",
    "HBA_BINS = [0.0, 1.0, 2.0, 3.0] # =0(0), =1(1), =2(2), =3(3), >3(4)\n",
    "ROT_BONDS_BINS = [0.0, 1.0, 2.0, 3.0] # =0(0), =1(1), =2(2), =3(3), >3(4)\n",
    "R_VALUE_BINS = [0.05, 0.50] # <0.05(0), 0.05-0.50(1), >0.50(2)\n",
    "\n",
    "def load_and_process_data(filepath, stoi, max_len=BLOCK_SIZE, cache_file=DATA_CACHE_FILE):\n",
    "    \"\"\"\n",
    "    Charge les SMILES, calcule TOUTES les catégories et crée les tenseurs.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Chargement des données catégorielles étendues depuis le cache '{cache_file}'...\")\n",
    "        data = torch.load(cache_file)\n",
    "        print(\"Données chargées.\")\n",
    "        return data\n",
    "\n",
    "    print(\"Traitement des données SMILES (calcul de TOUTES les catégories)...\")\n",
    "    print(\"(Cela peut prendre du temps sur tout le dataset)\")\n",
    "    data_processed = []\n",
    "    pad_idx = stoi[PAD_TOKEN]\n",
    "    start_idx = stoi[START_TOKEN]\n",
    "    end_idx = stoi[END_TOKEN]\n",
    "\n",
    "    # Listes pour les statistiques et plots\n",
    "    all_logp = []\n",
    "    all_mw = []\n",
    "    all_hbd = []\n",
    "    all_hba = []\n",
    "    all_rot_bonds = []\n",
    "    all_aromatic_rings = []\n",
    "    all_non_aromatic_rings = []\n",
    "    all_functional_groups = []\n",
    "    all_r_values = []\n",
    "    all_lipinski = []\n",
    "\n",
    "    try:\n",
    "        # Compter les lignes pour tqdm\n",
    "        num_lines = sum(1 for line in open(filepath, 'r', encoding='utf-8'))\n",
    "        with open(filepath, 'r') as f:\n",
    "            for i, line in enumerate(tqdm(f, total=num_lines, desc=\"Calcul des catégories étendues\")):\n",
    "                smiles = line.strip()\n",
    "                # Vérifier les caractères avant MolFromSmiles\n",
    "                if not all(c in stoi for c in smiles) or len(smiles) > max_len - 2:\n",
    "                    continue\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is None:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Calcul de toutes les propriétés\n",
    "                    logp = Descriptors.MolLogP(mol)\n",
    "                    mw = Descriptors.MolWt(mol)\n",
    "                    hbd = Descriptors.NumHDonors(mol)\n",
    "                    hba = Descriptors.NumHAcceptors(mol)\n",
    "                    rot_bonds = Descriptors.NumRotatableBonds(mol)\n",
    "\n",
    "                    # Calcul des cycles\n",
    "                    aromatic_rings = Lipinski.NumAromaticRings(mol)\n",
    "                    non_aromatic_rings = Lipinski.NumAliphaticRings(mol)\n",
    "\n",
    "                    # Groupes fonctionnels\n",
    "                    functional_group = has_functional_group(mol)\n",
    "\n",
    "                    # R-value\n",
    "                    r_value = calculate_r_value(mol)\n",
    "\n",
    "                    # Compliance Lipinski\n",
    "                    lipinski_compliant = check_lipinski_compliance(mol)\n",
    "\n",
    "                    # Collecter les données pour les plots\n",
    "                    all_logp.append(logp)\n",
    "                    all_mw.append(mw)\n",
    "                    all_hbd.append(hbd)\n",
    "                    all_hba.append(hba)\n",
    "                    all_rot_bonds.append(rot_bonds)\n",
    "                    all_aromatic_rings.append(aromatic_rings)\n",
    "                    all_non_aromatic_rings.append(non_aromatic_rings)\n",
    "                    all_functional_groups.append(functional_group)\n",
    "                    all_r_values.append(r_value)\n",
    "                    all_lipinski.append(lipinski_compliant)\n",
    "\n",
    "                    # Conversion en catégories\n",
    "                    logp_cat = get_category(logp, LOGP_BINS)\n",
    "                    mw_cat = get_category(mw, MW_BINS)\n",
    "                    hbd_cat = get_category(hbd, HBD_BINS)\n",
    "                    hba_cat = get_category(hba, HBA_BINS)\n",
    "                    rot_bonds_cat = get_category(rot_bonds, ROT_BONDS_BINS)\n",
    "\n",
    "                    # R-value catégorielle\n",
    "                    if r_value < 0.05:\n",
    "                        r_value_cat = 0.0\n",
    "                    elif r_value <= 0.50:\n",
    "                        r_value_cat = 1.0\n",
    "                    else:\n",
    "                        r_value_cat = 2.0\n",
    "\n",
    "                    # Vecteur de condition étendu (10 dimensions)\n",
    "                    condition_vector = torch.tensor([\n",
    "                        logp_cat, mw_cat, hbd_cat, hba_cat, rot_bonds_cat,\n",
    "                        float(aromatic_rings), float(non_aromatic_rings),\n",
    "                        functional_group, r_value_cat, lipinski_compliant\n",
    "                    ], dtype=torch.float32)\n",
    "\n",
    "                    token_ids = [start_idx] + encode(smiles) + [end_idx]\n",
    "                    seq_len = len(token_ids)\n",
    "                    x = torch.full((max_len,), pad_idx, dtype=torch.long)\n",
    "                    y = torch.full((max_len,), pad_idx, dtype=torch.long)\n",
    "                    x[:seq_len] = torch.tensor(token_ids, dtype=torch.long)\n",
    "                    y[:seq_len-1] = torch.tensor(token_ids[1:], dtype=torch.long)\n",
    "\n",
    "                    data_processed.append((x, y, condition_vector))\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Gérer les erreurs RDKit spécifiques au calcul\n",
    "                    continue\n",
    "\n",
    "                # Appel périodique au garbage collector\n",
    "                if i % 50000 == 0:\n",
    "                    gc.collect()\n",
    "\n",
    "        # Créer et sauvegarder les plots des distributions étendues\n",
    "        create_extended_data_distribution_plots(\n",
    "            all_logp, all_mw, all_hbd, all_hba, all_rot_bonds,\n",
    "            all_aromatic_rings, all_non_aromatic_rings,\n",
    "            all_functional_groups, all_r_values, all_lipinski\n",
    "        )\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERREUR: Le fichier de données '{filepath}' est introuvable.\")\n",
    "        exit()\n",
    "    except MemoryError:\n",
    "        print(\"\\nERREUR: Manque de mémoire pour traiter tout le dataset.\")\n",
    "        print(\"Vous devrez peut-être utiliser un échantillon plus petit ou augmenter la RAM.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"\\nNombre total de molécules valides chargées : {len(data_processed)}\")\n",
    "    print(f\"Sauvegarde des données traitées dans le cache '{cache_file}'...\")\n",
    "    torch.save(data_processed, cache_file)\n",
    "    print(\"Données sauvegardées.\")\n",
    "\n",
    "    return data_processed\n",
    "\n",
    "def create_extended_data_distribution_plots(logp_data, mw_data, hbd_data, hba_data, rot_bonds_data,\n",
    "                                          aromatic_rings_data, non_aromatic_rings_data,\n",
    "                                          functional_groups_data, r_values_data, lipinski_data):\n",
    "    \"\"\"Crée et sauvegarde les plots de distribution des données étendues\"\"\"\n",
    "    print(\"Création des plots de distribution des données étendues...\")\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "    # Plot LogP\n",
    "    axes[0, 0].hist(logp_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('LogP')\n",
    "    axes[0, 0].set_ylabel('Fréquence')\n",
    "    axes[0, 0].set_title('Distribution de LogP')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot MW\n",
    "    axes[0, 1].hist(mw_data, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Poids Moléculaire (MW)')\n",
    "    axes[0, 1].set_ylabel('Fréquence')\n",
    "    axes[0, 1].set_title('Distribution du Poids Moléculaire')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot HBD\n",
    "    axes[0, 2].hist(hbd_data, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 2].set_xlabel('Nombre de Donneurs H (HBD)')\n",
    "    axes[0, 2].set_ylabel('Fréquence')\n",
    "    axes[0, 2].set_title('Distribution des Donneurs H')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot HBA\n",
    "    axes[1, 0].hist(hba_data, bins=20, alpha=0.7, color='gold', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Nombre d\\'Accepteurs H (HBA)')\n",
    "    axes[1, 0].set_ylabel('Fréquence')\n",
    "    axes[1, 0].set_title('Distribution des Accepteurs H')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot Rotatable Bonds\n",
    "    axes[1, 1].hist(rot_bonds_data, bins=20, alpha=0.7, color='violet', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Liaisons Rotatives')\n",
    "    axes[1, 1].set_ylabel('Fréquence')\n",
    "    axes[1, 1].set_title('Distribution des Liaisons Rotatives')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot Aromatic Rings\n",
    "    axes[1, 2].hist(aromatic_rings_data, bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 2].set_xlabel('Cycles Aromatiques')\n",
    "    axes[1, 2].set_ylabel('Fréquence')\n",
    "    axes[1, 2].set_title('Distribution des Cycles Aromatiques')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot Non-Aromatic Rings\n",
    "    axes[2, 0].hist(non_aromatic_rings_data, bins=10, alpha=0.7, color='cyan', edgecolor='black')\n",
    "    axes[2, 0].set_xlabel('Cycles Non-Aromatiques')\n",
    "    axes[2, 0].set_ylabel('Fréquence')\n",
    "    axes[2, 0].set_title('Distribution des Cycles Non-Aromatiques')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot R-values\n",
    "    axes[2, 1].hist(r_values_data, bins=50, alpha=0.7, color='brown', edgecolor='black')\n",
    "    axes[2, 1].set_xlabel('R-value')\n",
    "    axes[2, 1].set_ylabel('Fréquence')\n",
    "    axes[2, 1].set_title('Distribution des R-values')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot Lipinski Compliance\n",
    "    lipinski_counts = [sum(lipinski_data), len(lipinski_data) - sum(lipinski_data)]\n",
    "    axes[2, 2].bar(['Compliant', 'Non-Compliant'], lipinski_counts,\n",
    "                   color=['green', 'red'], alpha=0.7)\n",
    "    axes[2, 2].set_ylabel('Nombre de Molécules')\n",
    "    axes[2, 2].set_title('Compliance Lipinski')\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(PLOTS_DIR, 'extended_data_distributions.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✓ Plots de distribution étendus sauvegardés dans : {plot_path}\")\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# --- PARTIE 4 : ARCHITECTURE DU MODÈLE (Avec dimension de condition étendue) ---\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    vocab_size: int = vocab_size # Utilise la variable globale\n",
    "    n_layer: int = N_LAYER\n",
    "    n_head: int = N_HEAD\n",
    "    n_embd: int = N_EMBD\n",
    "    dropout: float = DROPOUT\n",
    "    condition_dim: int = CONDITION_DIM # Maintenant 10 dimensions\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- PARTIE 5 : LE TRANSFORMATEUR CONDITIONNEL (Avec condition étendue) ---\n",
    "\n",
    "class ConditionalDrugGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "\n",
    "        self.condition_projector = nn.Sequential(\n",
    "            nn.Linear(config.condition_dim, config.n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, conditions=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Séquence trop longue: {T}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "\n",
    "        assert conditions is not None, \"Les conditions doivent être fournies !\"\n",
    "        cond_emb = self.condition_projector(conditions)\n",
    "\n",
    "        x = self.transformer.drop(tok_emb + pos_emb + cond_emb.unsqueeze(1))\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=stoi[PAD_TOKEN])\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "# --- PARTIE 6 : FONCTIONS DE CHECKPOINT ET D'ÉVALUATION (Avec Plots) ---\n",
    "\n",
    "def save_checkpoint(model, optimizer, iter_num, best_val_loss, config, filepath):\n",
    "    print(f\"Sauvegarde du checkpoint dans {filepath}...\")\n",
    "    torch.save({\n",
    "        'iter_num': iter_num,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': config,\n",
    "    }, filepath)\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"Aucun checkpoint trouvé. Démarrage d'un nouvel entraînement.\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "    print(f\"Chargement du checkpoint depuis {filepath}...\")\n",
    "    checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Reprise de l'entraînement à l'itération {iter_num} (meilleure perte val: {best_val_loss:.4f})\")\n",
    "    return iter_num, best_val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader, eval_iters=EVAL_ITERS):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        loader = train_loader if split == 'train' else val_loader\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        loader_iter = iter(loader)\n",
    "        for k in range(eval_iters):\n",
    "            try:\n",
    "                x, y, c = next(loader_iter)\n",
    "            except StopIteration:\n",
    "                loader_iter = iter(loader)\n",
    "                x, y, c = next(loader_iter)\n",
    "\n",
    "            x, y, c = x.to(DEVICE), y.to(DEVICE), c.to(DEVICE)\n",
    "            logits, loss = model(x, y, c)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Variables globales pour suivre l'historique des pertes\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "iterations = []\n",
    "\n",
    "def plot_training_progress(iter_num, train_loss, val_loss):\n",
    "    \"\"\"Crée et sauvegarde le plot de progression de l'entraînement\"\"\"\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    iterations.append(iter_num)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, train_losses, 'b-', label='Perte d\\'entraînement', alpha=0.7)\n",
    "    plt.plot(iterations, val_losses, 'r-', label='Perte de validation', alpha=0.7)\n",
    "    plt.xlabel('Itérations')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.title('Progression de l\\'Entraînement')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Sauvegarder le plot\n",
    "    plot_path = os.path.join(PLOTS_DIR, f'training_progress_iter_{iter_num}.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✓ Plot d'entraînement sauvegardé : {plot_path}\")\n",
    "\n",
    "# --- FONCTION DE GÉNÉRATION CORRIGÉE (Pour conditions étendues) ---\n",
    "@torch.no_grad()\n",
    "def generate_conditional(model, condition_tensor, stoi, itos, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Génère une séquence SMILES à partir d'un tenseur de condition CATÉGORIELLE ÉTENDUE.\n",
    "    CORRECTION : Exclut le token <start> du décodage final.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_idx = stoi[START_TOKEN]\n",
    "    end_idx = stoi[END_TOKEN]\n",
    "\n",
    "    idx = torch.tensor([[start_idx]], dtype=torch.long, device=DEVICE)\n",
    "    condition_tensor = condition_tensor.to(DEVICE)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "\n",
    "        # Passe le tenseur de catégories étendues directement\n",
    "        logits, _ = model(idx_cond, conditions=condition_tensor)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if idx_next.item() == end_idx:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # CORRECTION : Exclure le token de début et convertir les tokens en SMILES\n",
    "    generated_tokens = idx[0].tolist()\n",
    "\n",
    "    # Exclure le token <start> et s'arrêter au token <end> si présent\n",
    "    if len(generated_tokens) > 1:\n",
    "        # Commencer à partir du token après <start>\n",
    "        tokens_to_decode = generated_tokens[1:]\n",
    "\n",
    "        # S'arrêter au token <end> s'il est présent\n",
    "        if end_idx in tokens_to_decode:\n",
    "            end_pos = tokens_to_decode.index(end_idx)\n",
    "            tokens_to_decode = tokens_to_decode[:end_pos]\n",
    "    else:\n",
    "        tokens_to_decode = []\n",
    "\n",
    "    generated_smiles = decode(tokens_to_decode)\n",
    "\n",
    "    return generated_smiles\n",
    "\n",
    "# --- FONCTION DE VÉRIFICATION DES PROPRIÉTÉS ÉTENDUES ---\n",
    "def check_mol_all_props(smiles):\n",
    "    \"\"\" Vérifie TOUTES les propriétés réelles. \"\"\"\n",
    "    if not smiles:  # Vérifier si la chaîne est vide\n",
    "        return \"Vide\", 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0.0, 0\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return \"Invalide\", 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0.0, 0\n",
    "\n",
    "    # Calcul de toutes les propriétés\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    hbd = Descriptors.NumHDonors(mol)\n",
    "    hba = Descriptors.NumHAcceptors(mol)\n",
    "    rot_bonds = Descriptors.NumRotatableBonds(mol)\n",
    "    aromatic_rings = Lipinski.NumAromaticRings(mol)\n",
    "    non_aromatic_rings = Lipinski.NumAliphaticRings(mol)\n",
    "    functional_group = has_functional_group(mol)\n",
    "    r_value = calculate_r_value(mol)\n",
    "    lipinski_compliant = check_lipinski_compliance(mol)\n",
    "\n",
    "    return \"Valide\", logp, mw, hbd, hba, rot_bonds, aromatic_rings, non_aromatic_rings, functional_group, r_value, lipinski_compliant\n",
    "\n",
    "def plot_extended_generation_results(condition_results, condition_name):\n",
    "    \"\"\"Crée un plot des résultats de génération étendus pour une condition donnée\"\"\"\n",
    "    valid_molecules = [result for result in condition_results if result['status'] == 'Valide']\n",
    "\n",
    "    if not valid_molecules:\n",
    "        print(f\"Aucune molécule valide générée pour {condition_name}\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "    # Plot 1: Distribution des LogP générés\n",
    "    logp_values = [result['logp'] for result in valid_molecules]\n",
    "    axes[0, 0].hist(logp_values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('LogP')\n",
    "    axes[0, 0].set_ylabel('Nombre de Molécules')\n",
    "    axes[0, 0].set_title(f'Distribution LogP - {condition_name}')\n",
    "    axes[0, 0].axvline(x=3, color='red', linestyle='--', label='LogP ≤ 3')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "        # Plot 2: Distribution des MW générés\n",
    "    mw_values = [result['mw'] for result in valid_molecules]\n",
    "    axes[0, 1].hist(mw_values, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Poids Moléculaire')\n",
    "    axes[0, 1].set_ylabel('Nombre de Molécules')\n",
    "    axes[0, 1].set_title(f'Distribution MW - {condition_name}')\n",
    "    axes[0, 1].axvline(x=480, color='red', linestyle='--', label='MW ≤ 480')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Distribution des HBD générés\n",
    "    hbd_values = [result['hbd'] for result in valid_molecules]\n",
    "    axes[0, 2].hist(hbd_values, bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 2].set_xlabel('HBD')\n",
    "    axes[0, 2].set_ylabel('Nombre de Molécules')\n",
    "    axes[0, 2].set_title(f'Distribution HBD - {condition_name}')\n",
    "    axes[0, 2].axvline(x=3, color='red', linestyle='--', label='HBD ≤ 3')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Distribution des HBA générés\n",
    "    hba_values = [result['hba'] for result in valid_molecules]\n",
    "    axes[1, 0].hist(hba_values, bins=10, alpha=0.7, color='gold', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('HBA')\n",
    "    axes[1, 0].set_ylabel('Nombre de Molécules')\n",
    "    axes[1, 0].set_title(f'Distribution HBA - {condition_name}')\n",
    "    axes[1, 0].axvline(x=3, color='red', linestyle='--', label='HBA ≤ 3')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 5: Distribution des liaisons rotatives\n",
    "    rot_bonds_values = [result['rot_bonds'] for result in valid_molecules]\n",
    "    axes[1, 1].hist(rot_bonds_values, bins=10, alpha=0.7, color='violet', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Liaisons Rotatives')\n",
    "    axes[1, 1].set_ylabel('Nombre de Molécules')\n",
    "    axes[1, 1].set_title(f'Distribution Liaisons Rotatives - {condition_name}')\n",
    "    axes[1, 1].axvline(x=3, color='red', linestyle='--', label='RotBonds ≤ 3')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 6: Distribution des cycles aromatiques\n",
    "    aromatic_rings_values = [result['aromatic_rings'] for result in valid_molecules]\n",
    "    axes[1, 2].hist(aromatic_rings_values, bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 2].set_xlabel('Cycles Aromatiques')\n",
    "    axes[1, 2].set_ylabel('Nombre de Molécules')\n",
    "    axes[1, 2].set_title(f'Cycles Aromatiques - {condition_name}')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 7: Distribution des cycles non-aromatiques\n",
    "    non_aromatic_rings_values = [result['non_aromatic_rings'] for result in valid_molecules]\n",
    "    axes[2, 0].hist(non_aromatic_rings_values, bins=10, alpha=0.7, color='cyan', edgecolor='black')\n",
    "    axes[2, 0].set_xlabel('Cycles Non-Aromatiques')\n",
    "    axes[2, 0].set_ylabel('Nombre de Molécules')\n",
    "    axes[2, 0].set_title(f'Cycles Non-Aromatiques - {condition_name}')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 8: Distribution des R-values\n",
    "    r_value_values = [result['r_value'] for result in valid_molecules]\n",
    "    axes[2, 1].hist(r_value_values, bins=20, alpha=0.7, color='brown', edgecolor='black')\n",
    "    axes[2, 1].set_xlabel('R-value')\n",
    "    axes[2, 1].set_ylabel('Nombre de Molécules')\n",
    "    axes[2, 1].set_title(f'Distribution R-value - {condition_name}')\n",
    "    axes[2, 1].axvline(x=0.05, color='red', linestyle='--', label='R ≥ 0.05')\n",
    "    axes[2, 1].axvline(x=0.50, color='red', linestyle='--', label='R ≤ 0.50')\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 9: Compliance Lipinski\n",
    "    lipinski_counts = [sum(1 for r in valid_molecules if r['lipinski_compliant'] == 1),\n",
    "                      sum(1 for r in valid_molecules if r['lipinski_compliant'] == 0)]\n",
    "    axes[2, 2].bar(['Compliant', 'Non-Compliant'], lipinski_counts,\n",
    "                   color=['green', 'red'], alpha=0.7)\n",
    "    axes[2, 2].set_ylabel('Nombre de Molécules')\n",
    "    axes[2, 2].set_title(f'Compliance Lipinski - {condition_name}')\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(PLOTS_DIR, f'extended_generation_{condition_name.replace(\" \", \"_\")}.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✓ Plot de génération étendu sauvegardé : {plot_path}\")\n",
    "\n",
    "# --- PARTIE 7 : SCRIPT PRINCIPAL D'EXÉCUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1. Vocabulaire - stoi, itos, vocab_size sont déjà globales\n",
    "\n",
    "    # 2. Données (charge ou traite les données catégorielles étendues)\n",
    "    full_data = load_and_process_data(DATA_FILE, stoi, cache_file=DATA_CACHE_FILE)\n",
    "\n",
    "    # 3. DataLoaders\n",
    "    train_size = int(0.9 * len(full_data))\n",
    "    val_size = len(full_data) - train_size\n",
    "    train_data, val_data = torch.utils.data.random_split(full_data, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(SMILESDataset(train_data), batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(SMILESDataset(val_data), batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # 4. Modèle et Optimiseur\n",
    "    config = GPTConfig(vocab_size=vocab_size, condition_dim=CONDITION_DIM)\n",
    "    model = ConditionalDrugGPT(config)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    print(f\"Nombre de paramètres : {sum(p.numel() for p in model.parameters())/1e6:.2f} M\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # 5. Chargement du Checkpoint\n",
    "    start_iter, best_val_loss = load_checkpoint(CHECKPOINT_FILE, model, optimizer)\n",
    "\n",
    "    # 6. Boucle d'entraînement\n",
    "    print(f\"Début de l'entraînement sur {DEVICE}...\")\n",
    "    start_time = time.time()\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    for iter_num in range(start_iter, MAX_ITERS):\n",
    "\n",
    "        if iter_num > 0 and (iter_num % EVAL_INTERVAL == 0 or iter_num == MAX_ITERS - 1):\n",
    "            losses = estimate_loss(model, train_loader, val_loader)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Étape {iter_num}: perte train {losses['train']:.4f}, perte val {losses['val']:.4f}, temps {elapsed:.1f}s\")\n",
    "\n",
    "            # Mettre à jour le plot de progression\n",
    "            plot_training_progress(iter_num, losses['train'], losses['val'])\n",
    "\n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "                save_checkpoint(model, optimizer, iter_num, best_val_loss, config, CHECKPOINT_FILE)\n",
    "\n",
    "        try:\n",
    "            xb, yb, cb = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            xb, yb, cb = next(train_iter)\n",
    "\n",
    "        xb, yb, cb = xb.to(DEVICE), yb.to(DEVICE), cb.to(DEVICE)\n",
    "\n",
    "        logits, loss = model(xb, targets=yb, conditions=cb)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Appel périodique au garbage collector pendant l'entraînement\n",
    "        if iter_num % 1000 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"Entraînement terminé !\")\n",
    "\n",
    "    # 7. Génération avec TOUS les objectifs\n",
    "    print(\"\\n--- Génération de Molécules avec TOUS les Objectifs ---\")\n",
    "\n",
    "    # Test de génération simple avant les boucles\n",
    "    print(\"\\n=== TEST DE GÉNÉRATION SIMPLE ===\")\n",
    "\n",
    "    # Condition de test: LogP≤3, MW≤480, HBD≤3, HBA≤3, RotBonds≤3,\n",
    "    # 2 cycles aromatiques, 1 cycle non-aromatique, avec groupe fonctionnel,\n",
    "    # R-value [0.05-0.50], Lipinski compliant\n",
    "    target_cats_test = [\n",
    "        1.0,  # LogP ≤ 3 (catégorie 1)\n",
    "        1.0,  # MW ≤ 480 (catégorie 1)\n",
    "        3.0,  # HBD ≤ 3 (catégorie 3)\n",
    "        3.0,  # HBA ≤ 3 (catégorie 3)\n",
    "        3.0,  # RotBonds ≤ 3 (catégorie 3)\n",
    "        2.0,  # 2 cycles aromatiques\n",
    "        1.0,  # 1 cycle non-aromatique\n",
    "        1.0,  # Avec groupe fonctionnel\n",
    "        1.0,  # R-value dans [0.05-0.50]\n",
    "        1.0   # Lipinski compliant\n",
    "    ]\n",
    "\n",
    "    condition_tensor_test = torch.tensor(target_cats_test, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    test_smiles = generate_conditional(model, condition_tensor_test, stoi, itos, max_new_tokens=50, top_k=10)\n",
    "    valid, logp, mw, hbd, hba, rot_bonds, aromatic_rings, non_aromatic_rings, functional_group, r_value, lipinski = check_mol_all_props(test_smiles)\n",
    "    print(f\"Test génération: '{test_smiles}'\")\n",
    "    print(f\"  Statut: {valid}\")\n",
    "    print(f\"  LogP: {logp:.1f}, MW: {mw:.0f}, HBD: {hbd}, HBA: {hba}\")\n",
    "    print(f\"  RotBonds: {rot_bonds}, AromaticRings: {aromatic_rings}, NonAromaticRings: {non_aromatic_rings}\")\n",
    "    print(f\"  FunctionalGroup: {functional_group}, R-value: {r_value:.3f}, Lipinski: {lipinski}\")\n",
    "\n",
    "    # Condition 1: Objectif 1 - LogP ≤ 3 seul\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OBJECTIF 1: LogP ≤ 3 (Rule of Three - composante unique)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    target_cats_1 = [\n",
    "        1.0,  # LogP ≤ 3 (catégorie 1)\n",
    "        0.0,  # MW quelconque\n",
    "        0.0,  # HBD quelconque\n",
    "        0.0,  # HBA quelconque\n",
    "        0.0,  # RotBonds quelconque\n",
    "        0.0,  # Cycles aromatiques quelconques\n",
    "        0.0,  # Cycles non-aromatiques quelconques\n",
    "        0.0,  # Groupe fonctionnel quelconque\n",
    "        0.0,  # R-value quelconque\n",
    "        0.0   # Lipinski quelconque\n",
    "    ]\n",
    "\n",
    "    condition_tensor_1 = torch.tensor(target_cats_1, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    print(f\"Génération pour LogP ≤ 3 uniquement\")\n",
    "    condition_1_results = []\n",
    "    valid_count = 0\n",
    "    for i in range(120):\n",
    "        mol_str = generate_conditional(model, condition_tensor_1, stoi, itos, max_new_tokens=50, top_k=10)\n",
    "        valid, logp, mw, hbd, hba, rot_bonds, aromatic_rings, non_aromatic_rings, functional_group, r_value, lipinski = check_mol_all_props(mol_str)\n",
    "\n",
    "        result = {\n",
    "            'smiles': mol_str,\n",
    "            'status': valid,\n",
    "            'logp': logp,\n",
    "            'mw': mw,\n",
    "            'hbd': hbd,\n",
    "            'hba': hba,\n",
    "            'rot_bonds': rot_bonds,\n",
    "            'aromatic_rings': aromatic_rings,\n",
    "            'non_aromatic_rings': non_aromatic_rings,\n",
    "            'functional_group': functional_group,\n",
    "            'r_value': r_value,\n",
    "            'lipinski_compliant': lipinski\n",
    "        }\n",
    "        condition_1_results.append(result)\n",
    "\n",
    "        if i < 5:  # Afficher seulement les 5 premiers\n",
    "            print(f\"  {i+1}. -> '{mol_str}'\")\n",
    "            print(f\"     (Valide: {valid}, LogP: {logp:.1f})\")\n",
    "\n",
    "        if valid == \"Valide\":\n",
    "            valid_count += 1\n",
    "\n",
    "    print(f\"Molécules valides générées: {valid_count}/120\")\n",
    "    print(f\"Parmi les valides, LogP ≤ 3: {sum(1 for r in condition_1_results if r['status'] == 'Valide' and r['logp'] <= 3)}\")\n",
    "\n",
    "    # Créer le plot pour la condition 1\n",
    "    plot_extended_generation_results(condition_1_results, \"Objectif 1 - LogP ≤ 3\")\n",
    "\n",
    "    # Condition 2: Objectif 2 - Multiples objectifs structuraux [13]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OBJECTIF 2: Multiples objectifs structuraux [13]\")\n",
    "    print(\"  (i) 2 cycles aromatiques, 1 cycle non-aromatique\")\n",
    "    print(\"  (ii) Au moins un groupe fonctionnel: -OH, -COOR, -COOH, ou -NH2\")\n",
    "    print(\"  (iii) R-value dans [0.05–0.50]\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    target_cats_2 = [\n",
    "        0.0,  # LogP quelconque\n",
    "        0.0,  # MW quelconque\n",
    "        0.0,  # HBD quelconque\n",
    "        0.0,  # HBA quelconque\n",
    "        0.0,  # RotBonds quelconque\n",
    "        2.0,  # 2 cycles aromatiques\n",
    "        1.0,  # 1 cycle non-aromatique\n",
    "        1.0,  # Avec groupe fonctionnel\n",
    "        1.0,  # R-value dans [0.05-0.50]\n",
    "        0.0   # Lipinski quelconque\n",
    "    ]\n",
    "\n",
    "    condition_tensor_2 = torch.tensor(target_cats_2, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    print(f\"Génération pour objectifs structuraux [13]\")\n",
    "    condition_2_results = []\n",
    "    valid_count = 0\n",
    "    for i in range(120):\n",
    "        mol_str = generate_conditional(model, condition_tensor_2, stoi, itos, max_new_tokens=50, top_k=10)\n",
    "        valid, logp, mw, hbd, hba, rot_bonds, aromatic_rings, non_aromatic_rings, functional_group, r_value, lipinski = check_mol_all_props(mol_str)\n",
    "\n",
    "        result = {\n",
    "            'smiles': mol_str,\n",
    "            'status': valid,\n",
    "            'logp': logp,\n",
    "            'mw': mw,\n",
    "            'hbd': hbd,\n",
    "            'hba': hba,\n",
    "            'rot_bonds': rot_bonds,\n",
    "            'aromatic_rings': aromatic_rings,\n",
    "            'non_aromatic_rings': non_aromatic_rings,\n",
    "            'functional_group': functional_group,\n",
    "            'r_value': r_value,\n",
    "            'lipinski_compliant': lipinski\n",
    "        }\n",
    "        condition_2_results.append(result)\n",
    "\n",
    "        if i < 5:  # Afficher seulement les 5 premiers\n",
    "            print(f\"  {i+1}. -> '{mol_str}'\")\n",
    "            print(f\"     (Valide: {valid}, Aromatic: {aromatic_rings}, NonAromatic: {non_aromatic_rings}, Functional: {functional_group}, R-value: {r_value:.3f})\")\n",
    "\n",
    "        if valid == \"Valide\":\n",
    "            valid_count += 1\n",
    "\n",
    "    # Calcul des statistiques pour l'objectif 2\n",
    "    valid_mols_2 = [r for r in condition_2_results if r['status'] == 'Valide']\n",
    "    if valid_mols_2:\n",
    "        target_aromatic = sum(1 for r in valid_mols_2 if r['aromatic_rings'] == 2)\n",
    "        target_non_aromatic = sum(1 for r in valid_mols_2 if r['non_aromatic_rings'] == 1)\n",
    "        target_functional = sum(1 for r in valid_mols_2 if r['functional_group'] == 1)\n",
    "        target_r_value = sum(1 for r in valid_mols_2 if 0.05 <= r['r_value'] <= 0.50)\n",
    "\n",
    "        print(f\"Molécules valides générées: {valid_count}/120\")\n",
    "        print(f\"Cycles aromatiques = 2: {target_aromatic}/{valid_count}\")\n",
    "        print(f\"Cycles non-aromatiques = 1: {target_non_aromatic}/{valid_count}\")\n",
    "        print(f\"Avec groupe fonctionnel: {target_functional}/{valid_count}\")\n",
    "        print(f\"R-value dans [0.05-0.50]: {target_r_value}/{valid_count}\")\n",
    "\n",
    "    # Créer le plot pour la condition 2\n",
    "    plot_extended_generation_results(condition_2_results, \"Objectif 2 - Structuraux [13]\")\n",
    "\n",
    "    # Condition 3: Objectif 3 - Règle complète de Lipinski\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OBJECTIF 3: Règle complète de Lipinski (Rule of Three)\")\n",
    "    print(\"  - LogP ≤ 3\")\n",
    "    print(\"  - MW ≤ 480 g/mol\")\n",
    "    print(\"  - HBD ≤ 3\")\n",
    "    print(\"  - HBA ≤ 3\")\n",
    "    print(\"  - Liaisons rotatives ≤ 3\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    target_cats_3 = [\n",
    "        1.0,  # LogP ≤ 3\n",
    "        1.0,  # MW ≤ 480\n",
    "        3.0,  # HBD ≤ 3\n",
    "        3.0,  # HBA ≤ 3\n",
    "        3.0,  # RotBonds ≤ 3\n",
    "        0.0,  # Cycles aromatiques quelconques\n",
    "        0.0,  # Cycles non-aromatiques quelconques\n",
    "        0.0,  # Groupe fonctionnel quelconque\n",
    "        0.0,  # R-value quelconque\n",
    "        1.0   # Lipinski compliant\n",
    "    ]\n",
    "\n",
    "    condition_tensor_3 = torch.tensor(target_cats_3, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    print(f\"Génération pour règle complète de Lipinski\")\n",
    "    condition_3_results = []\n",
    "    valid_count = 0\n",
    "    for i in range(120):\n",
    "        mol_str = generate_conditional(model, condition_tensor_3, stoi, itos, max_new_tokens=50, top_k=10)\n",
    "        valid, logp, mw, hbd, hba, rot_bonds, aromatic_rings, non_aromatic_rings, functional_group, r_value, lipinski = check_mol_all_props(mol_str)\n",
    "\n",
    "        result = {\n",
    "            'smiles': mol_str,\n",
    "            'status': valid,\n",
    "            'logp': logp,\n",
    "            'mw': mw,\n",
    "            'hbd': hbd,\n",
    "            'hba': hba,\n",
    "            'rot_bonds': rot_bonds,\n",
    "            'aromatic_rings': aromatic_rings,\n",
    "            'non_aromatic_rings': non_aromatic_rings,\n",
    "            'functional_group': functional_group,\n",
    "            'r_value': r_value,\n",
    "            'lipinski_compliant': lipinski\n",
    "        }\n",
    "        condition_3_results.append(result)\n",
    "\n",
    "        if i < 5:  # Afficher seulement les 5 premiers\n",
    "            print(f\"  {i+1}. -> '{mol_str}'\")\n",
    "            print(f\"     (Valide: {valid}, LogP: {logp:.1f}, MW: {mw:.0f}, HBD: {hbd}, HBA: {hba}, RotBonds: {rot_bonds})\")\n",
    "\n",
    "        if valid == \"Valide\":\n",
    "            valid_count += 1\n",
    "\n",
    "    # Calcul des statistiques pour l'objectif 3\n",
    "    valid_mols_3 = [r for r in condition_3_results if r['status'] == 'Valide']\n",
    "    if valid_mols_3:\n",
    "        lipinski_compliant = sum(1 for r in valid_mols_3 if r['lipinski_compliant'] == 1)\n",
    "\n",
    "        print(f\"Molécules valides générées: {valid_count}/120\")\n",
    "        print(f\"Compliantes Lipinski: {lipinski_compliant}/{valid_count}\")\n",
    "\n",
    "    # Créer le plot pour la condition 3\n",
    "    plot_extended_generation_results(condition_3_results, \"Objectif 3 - Lipinski Complet\")\n",
    "\n",
    "    # Condition 4: TOUS les objectifs combinés\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OBJECTIF COMBINÉ: Tous les objectifs simultanément\")\n",
    "    print(\"  - LogP ≤ 3\")\n",
    "    print(\"  - MW ≤ 480 g/mol\")\n",
    "    print(\"  - HBD ≤ 3, HBA ≤ 3, RotBonds ≤ 3\")\n",
    "    print(\"  - 2 cycles aromatiques, 1 cycle non-aromatique\")\n",
    "    print(\"  - Au moins un groupe fonctionnel\")\n",
    "    print(\"  - R-value dans [0.05–0.50]\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    target_cats_4 = [\n",
    "        1.0,  # LogP ≤ 3\n",
    "        1.0,  # MW ≤ 480\n",
    "        3.0,  # HBD ≤ 3\n",
    "        3.0,  # HBA ≤ 3\n",
    "        3.0,  # RotBonds ≤ 3\n",
    "        2.0,  # 2 cycles aromatiques\n",
    "        1.0,  # 1 cycle non-aromatique\n",
    "        1.0,  # Avec groupe fonctionnel\n",
    "        1.0,  # R-value dans [0.05-0.50]\n",
    "        1.0   # Lipinski compliant\n",
    "    ]\n",
    "\n",
    "    condition_tensor_4 = torch.tensor(target_cats_4, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    print(f\"Génération pour TOUS les objectifs combinés\")\n",
    "    condition_4_results = []\n",
    "    valid_count = 0\n",
    "    for i in range(120):\n",
    "        mol_str = generate_conditional(model, condition_tensor_4, stoi, itos, max_new_tokens=50, top_k=10)\n",
    "        valid, logp, mw, hbd, hba, rot_bonds, aromatic_rings, non_aromatic_rings, functional_group, r_value, lipinski = check_mol_all_props(mol_str)\n",
    "\n",
    "        result = {\n",
    "            'smiles': mol_str,\n",
    "            'status': valid,\n",
    "            'logp': logp,\n",
    "            'mw': mw,\n",
    "            'hbd': hbd,\n",
    "            'hba': hba,\n",
    "            'rot_bonds': rot_bonds,\n",
    "            'aromatic_rings': aromatic_rings,\n",
    "            'non_aromatic_rings': non_aromatic_rings,\n",
    "            'functional_group': functional_group,\n",
    "            'r_value': r_value,\n",
    "            'lipinski_compliant': lipinski\n",
    "        }\n",
    "        condition_4_results.append(result)\n",
    "\n",
    "        if i < 5:  # Afficher seulement les 5 premiers\n",
    "            print(f\"  {i+1}. -> '{mol_str}'\")\n",
    "            print(f\"     (Valide: {valid})\")\n",
    "\n",
    "        if valid == \"Valide\":\n",
    "            valid_count += 1\n",
    "\n",
    "    # Calcul des statistiques pour l'objectif combiné\n",
    "    valid_mols_4 = [r for r in condition_4_results if r['status'] == 'Valide']\n",
    "    if valid_mols_4:\n",
    "        all_targets_met = sum(1 for r in valid_mols_4 if (\n",
    "            r['logp'] <= 3 and\n",
    "            r['mw'] <= 480 and\n",
    "            r['hbd'] <= 3 and\n",
    "            r['hba'] <= 3 and\n",
    "            r['rot_bonds'] <= 3 and\n",
    "            r['aromatic_rings'] == 2 and\n",
    "            r['non_aromatic_rings'] == 1 and\n",
    "            r['functional_group'] == 1 and\n",
    "            0.05 <= r['r_value'] <= 0.50\n",
    "        ))\n",
    "\n",
    "        print(f\"Molécules valides générées: {valid_count}/120\")\n",
    "        print(f\"Tous les objectifs atteints: {all_targets_met}/{valid_count}\")\n",
    "\n",
    "    # Créer le plot pour la condition 4\n",
    "    plot_extended_generation_results(condition_4_results, \"Objectif Combiné - Tous Critères\")\n",
    "\n",
    "    # Résumé final\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RÉSUMÉ FINAL DES RÉSULTATS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    all_results = [\n",
    "        (\"Objectif 1 - LogP ≤ 3\", condition_1_results),\n",
    "        (\"Objectif 2 - Structuraux [13]\", condition_2_results),\n",
    "        (\"Objectif 3 - Lipinski Complet\", condition_3_results),\n",
    "        (\"Objectif Combiné - Tous Critères\", condition_4_results)\n",
    "    ]\n",
    "\n",
    "    for name, results in all_results:\n",
    "        valid_mols = [r for r in results if r['status'] == 'Valide']\n",
    "        total = len(results)\n",
    "        valid_count = len(valid_mols)\n",
    "\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  - Molécules valides: {valid_count}/{total} ({valid_count/total*100:.1f}%)\")\n",
    "\n",
    "        if valid_mols:\n",
    "            avg_logp = sum(r['logp'] for r in valid_mols) / len(valid_mols)\n",
    "            avg_mw = sum(r['mw'] for r in valid_mols) / len(valid_mols)\n",
    "            print(f\"  - LogP moyen: {avg_logp:.2f}\")\n",
    "            print(f\"  - MW moyen: {avg_mw:.1f}\")\n",
    "            print(f\"  - Compliance Lipinski: {sum(r['lipinski_compliant'] for r in valid_mols)}/{len(valid_mols)}\")\n",
    "\n",
    "    print(f\"\\n✓ Tous les fichiers sauvegardés dans : {DRIVE_PATH}\")\n",
    "    print(\"✓ Entraînement et génération terminés avec succès !\")\n",
    "    print(\"✓ Tous les objectifs ont été intégrés et testés !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b03cff3",
   "metadata": {},
   "source": [
    "Temperature sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6620a",
   "metadata": {},
   "source": [
    "    condition_vector = CONDITIONS[4][\"get_vector\"](None) \n",
    "    here 4 represent the generation with the condition 4 to generate with cond 1 that verfify first objective write 1 instead and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "SCRIPT : ANALYSE DE SENSIBILITÉ TEMPÉRATURE - CONDITION 4\n",
    "Trouve la température optimale pour l'équilibre validité/novelty/unicité/IntDiv avec Condition 4\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, DataStructs, Crippen, Lipinski\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DRIVE_PATH = '/content/drive/MyDrive/cond_gpt_model1'\n",
    "VOCAB_FILE = os.path.join(DRIVE_PATH, 'vocab_dataset.json')\n",
    "CHECKPOINT_FILE = os.path.join(DRIVE_PATH, 'checkpoints', 'cond_gpt_categorical_extended.pth')\n",
    "DATA_FILE = '/content/drive/MyDrive/cond_gpt_model1/s_100_str_+1M_fixed.txt'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- FONCTIONS IDENTIQUES ---\n",
    "def has_functional_group(mol):\n",
    "    smarts_patterns = [\n",
    "        '[OH]',\n",
    "        '[#6]C(=O)[O;H0]',\n",
    "        'C(=O)[OH]',\n",
    "        '[NH2]'\n",
    "    ]\n",
    "    for pattern in smarts_patterns:\n",
    "        if mol.HasSubstructMatch(Chem.MolFromSmarts(pattern)):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def calculate_r_value(mol):\n",
    "    try:\n",
    "        mol_wt = Descriptors.MolWt(mol)\n",
    "        logp = Crippen.MolLogP(mol)\n",
    "        if mol_wt > 0:\n",
    "            r_value = logp / (mol_wt / 100)\n",
    "            return r_value\n",
    "        else:\n",
    "            return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# --- DÉFINITION DES CONDITIONS ---\n",
    "CONDITIONS = {\n",
    "    4: {\n",
    "        \"name\": \"Condition 4: Structural + Lipinski\",\n",
    "        \"description\": \"Combination of conditions 2 and 3\",\n",
    "        \"get_vector\": lambda mol: [1.0, 1.0, 3.0, 3.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- ARCHITECTURE MODÈLE ---\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = 57\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.1\n",
    "    condition_dim: int = 10\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class ConditionalDrugGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.condition_projector = nn.Sequential(\n",
    "            nn.Linear(config.condition_dim, config.n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, conditions=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Séquence trop longue: {T}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        assert conditions is not None, \"Les conditions doivent être fournies !\"\n",
    "        cond_emb = self.condition_projector(conditions)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb + cond_emb.unsqueeze(1))\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "# --- FONCTIONS POUR CALCULER LES MÉTRIQUES ---\n",
    "def calculate_fingerprint(mol):\n",
    "    \"\"\"Calcule le fingerprint Morgan pour une molécule\"\"\"\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "        return fp\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_intdiv(smiles_list):\n",
    "    \"\"\"Calcule la diversité interne (IntDiv) d'une liste de SMILES\"\"\"\n",
    "    if len(smiles_list) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    fingerprints = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            fp = calculate_fingerprint(mol)\n",
    "            if fp is not None:\n",
    "                fingerprints.append(fp)\n",
    "\n",
    "    if len(fingerprints) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    similarities = []\n",
    "    for i in range(len(fingerprints)):\n",
    "        for j in range(i+1, len(fingerprints)):\n",
    "            similarity = DataStructs.TanimotoSimilarity(fingerprints[i], fingerprints[j])\n",
    "            similarities.append(similarity)\n",
    "\n",
    "    if similarities:\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        intdiv = 1 - mean_similarity\n",
    "        return intdiv\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def calculate_novelty(generated_smiles, reference_smiles_file):\n",
    "    \"\"\"Calcule la nouveauté des molécules générées\"\"\"\n",
    "    try:\n",
    "        # Vérifier si le fichier existe\n",
    "        full_reference_path = os.path.join(DRIVE_PATH, reference_smiles_file)\n",
    "        if not os.path.exists(full_reference_path):\n",
    "            print(f\" Fichier de référence non trouvé: {full_reference_path}\")\n",
    "            print(f\" Recherché dans: {DRIVE_PATH}\")\n",
    "            return 100.0, len(generated_smiles)  # Si pas de référence, tout est nouveau\n",
    "\n",
    "        # Charger les SMILES de référence avec gestion d'erreurs\n",
    "        reference_smiles = set()\n",
    "        valid_reference_count = 0\n",
    "\n",
    "        print(f\" Chargement du fichier de référence: {reference_smiles_file}\")\n",
    "        with open(full_reference_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Valider que c'est un SMILES valide\n",
    "                    mol = Chem.MolFromSmiles(line)\n",
    "                    if mol:\n",
    "                        canon_smiles = Chem.MolToSmiles(mol)\n",
    "                        reference_smiles.add(canon_smiles)\n",
    "                        valid_reference_count += 1\n",
    "\n",
    "        print(f\"✓ SMILES de référence chargés: {len(reference_smiles)} (valides: {valid_reference_count})\")\n",
    "\n",
    "        if len(reference_smiles) == 0:\n",
    "            print(\"  Aucun SMILES valide dans le fichier de référence\")\n",
    "            return 100.0, len(generated_smiles)\n",
    "\n",
    "        # Calculer la nouveauté\n",
    "        novel_count = 0\n",
    "        valid_generated_count = 0\n",
    "\n",
    "        for smiles in tqdm(generated_smiles, desc=\"Calcul nouveauté\", leave=False):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                valid_generated_count += 1\n",
    "                canon_smiles = Chem.MolToSmiles(mol)\n",
    "                if canon_smiles not in reference_smiles:\n",
    "                    novel_count += 1\n",
    "\n",
    "        print(f\"✓ Molécules générées valides: {valid_generated_count}/{len(generated_smiles)}\")\n",
    "\n",
    "        if valid_generated_count == 0:\n",
    "            return 0.0, 0\n",
    "\n",
    "        novelty = (novel_count / valid_generated_count) * 100\n",
    "        return novelty, novel_count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors du calcul de la nouveauté: {e}\")\n",
    "        return 0.0, 0\n",
    "\n",
    "def calculate_uniqueness(smiles_list):\n",
    "    \"\"\"Calcule l'unicité des molécules générées\"\"\"\n",
    "    if not smiles_list:\n",
    "        return 0.0, 0\n",
    "\n",
    "    canonical_smiles = []\n",
    "    for smiles in tqdm(smiles_list, desc=\"Calcul unicité\", leave=False):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            canon_smiles = Chem.MolToSmiles(mol)\n",
    "            canonical_smiles.append(canon_smiles)\n",
    "\n",
    "    unique_smiles = set(canonical_smiles)\n",
    "    uniqueness = (len(unique_smiles) / len(smiles_list)) * 100 if smiles_list else 0.0\n",
    "    return uniqueness, len(unique_smiles)\n",
    "\n",
    "def calculate_validity(smiles_list):\n",
    "    \"\"\"Calcule la validité des molécules générées\"\"\"\n",
    "    if not smiles_list:\n",
    "        return 0.0, 0\n",
    "\n",
    "    valid_count = 0\n",
    "    for smiles in tqdm(smiles_list, desc=\"Calcul validité\", leave=False):\n",
    "        if Chem.MolFromSmiles(smiles) is not None:\n",
    "            valid_count += 1\n",
    "\n",
    "    validity = (valid_count / len(smiles_list)) * 100 if smiles_list else 0.0\n",
    "    return validity, valid_count\n",
    "\n",
    "def calculate_balanced_score(validity, novelty, uniqueness, intdiv):\n",
    "    \"\"\"\n",
    "    Calcule un score d'équilibre qui compromet entre les 4 métriques\n",
    "    Notre objectif est de trouver un compromis optimal entre:\n",
    "    - Validité (priorité haute)\n",
    "    - Novelty (priorité moyenne)\n",
    "    - Unicité (priorité moyenne)\n",
    "    - IntDiv (priorité moyenne)\n",
    "    \"\"\"\n",
    "    # Poids pour favoriser un bon équilibre\n",
    "    weights = {\n",
    "        'validity': 1.2,    # Légèrement plus important\n",
    "        'novelty': 1.0,\n",
    "        'uniqueness': 1.0,\n",
    "        'intdiv': 1.0\n",
    "    }\n",
    "\n",
    "    # Normaliser IntDiv (0-1 scale) pour le mettre à la même échelle que les pourcentages\n",
    "    intdiv_normalized = intdiv * 100  # Convertir en échelle 0-100\n",
    "\n",
    "    # Score pondéré qui favorise l'équilibre\n",
    "    if validity > 0 and novelty > 0 and uniqueness > 0 and intdiv_normalized > 0:\n",
    "        weighted_score = (\n",
    "            weights['validity'] * validity +\n",
    "            weights['novelty'] * novelty +\n",
    "            weights['uniqueness'] * uniqueness +\n",
    "            weights['intdiv'] * intdiv_normalized\n",
    "        ) / sum(weights.values())\n",
    "\n",
    "        # Pénaliser les déséquilibres extrêmes\n",
    "        metrics = [validity, novelty, uniqueness, intdiv_normalized]\n",
    "        std_penalty = np.std(metrics) * 0.1  # Pénalité douce pour les déséquilibres\n",
    "\n",
    "        balanced_score = weighted_score - std_penalty\n",
    "        return max(0, balanced_score)  # Assurer un score positif\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# --- GÉNÉRATION AVEC DIFFÉRENTES TEMPÉRATURES ---\n",
    "@torch.no_grad()\n",
    "def generate_with_temperature(model, condition_tensor, stoi, itos, start_idx, end_idx,\n",
    "                            temperature=0.5, num_molecules=1000):\n",
    "    \"\"\"Génère des molécules avec une température spécifique\"\"\"\n",
    "    generated_smiles = []\n",
    "\n",
    "    with tqdm(total=num_molecules, desc=f\"Temp {temperature}\", leave=False) as pbar:\n",
    "        while len(generated_smiles) < num_molecules:\n",
    "            # Paramètres\n",
    "            top_k = 30\n",
    "\n",
    "            # Génération\n",
    "            idx = torch.tensor([[start_idx]], dtype=torch.long, device=DEVICE)\n",
    "            condition_local = condition_tensor.to(DEVICE)\n",
    "\n",
    "            for step in range(80):\n",
    "                idx_cond = idx if idx.size(1) <= 128 else idx[:, -128:]\n",
    "                logits, _ = model(idx_cond, conditions=condition_local)\n",
    "                logits = logits[:, -1, :] / max(temperature, 0.1)\n",
    "\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                probs_mod = probs.clone()\n",
    "                probs_mod[0, start_idx] = 0.0\n",
    "                if step < 8:\n",
    "                    probs_mod[0, end_idx] = 0.0\n",
    "\n",
    "                if probs_mod.sum() > 0:\n",
    "                    probs_mod = probs_mod / probs_mod.sum()\n",
    "                else:\n",
    "                    probs_mod = probs\n",
    "\n",
    "                idx_next = torch.multinomial(probs_mod, num_samples=1)\n",
    "                next_token = idx_next.item()\n",
    "\n",
    "                if next_token == end_idx and step >= 8:\n",
    "                    break\n",
    "\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "            # Décodage\n",
    "            tokens = idx[0].tolist()\n",
    "            if len(tokens) > 1:\n",
    "                tokens_to_decode = tokens[1:]\n",
    "                if end_idx in tokens_to_decode:\n",
    "                    end_pos = tokens_to_decode.index(end_idx)\n",
    "                    tokens_to_decode = tokens_to_decode[:end_pos]\n",
    "            else:\n",
    "                tokens_to_decode = []\n",
    "\n",
    "            smiles = ''.join([itos[str(i)] for i in tokens_to_decode if str(i) in itos])\n",
    "\n",
    "            if smiles:\n",
    "                generated_smiles.append(smiles)\n",
    "                pbar.update(1)\n",
    "\n",
    "    return generated_smiles\n",
    "\n",
    "def evaluate_temperature_performance(model, condition_tensor, stoi, itos, start_idx, end_idx,\n",
    "                                   temperature, num_molecules=1000):\n",
    "    \"\"\"Évalue les performances pour une température donnée\"\"\"\n",
    "    # Génération\n",
    "    smiles_list = generate_with_temperature(\n",
    "        model, condition_tensor, stoi, itos, start_idx, end_idx,\n",
    "        temperature, num_molecules\n",
    "    )\n",
    "\n",
    "    print(f\"    Calcul des métriques pour température {temperature}...\")\n",
    "\n",
    "    # Calcul des métriques\n",
    "    validity, valid_count = calculate_validity(smiles_list)\n",
    "    novelty, novel_count = calculate_novelty(smiles_list, DATA_FILE)\n",
    "    uniqueness, unique_count = calculate_uniqueness(smiles_list)\n",
    "    intdiv = calculate_intdiv(smiles_list)\n",
    "\n",
    "    # Score d'équilibre\n",
    "    balanced_score = calculate_balanced_score(validity, novelty, uniqueness, intdiv)\n",
    "\n",
    "    return {\n",
    "        'temperature': temperature,\n",
    "        'validity': validity,\n",
    "        'novelty': novelty,\n",
    "        'uniqueness': uniqueness,\n",
    "        'intdiv': intdiv,\n",
    "        'balanced_score': balanced_score,\n",
    "        'valid_count': valid_count,\n",
    "        'novel_count': novel_count,\n",
    "        'unique_count': unique_count,\n",
    "        'total_generated': len(smiles_list)\n",
    "    }\n",
    "\n",
    "# --- ANALYSE DE SENSIBILITÉ ---\n",
    "def temperature_sensitivity_analysis(model, condition_tensor, stoi, itos, start_idx, end_idx):\n",
    "    \"\"\"Analyse la sensibilité aux différentes températures\"\"\"\n",
    "    print(\"🎯 ANALYSE DE SENSIBILITÉ - TEMPÉRATURE (CONDITION 4)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Objectif: Trouver un compromis optimal entre les 4 métriques\")\n",
    "    print(\"Condition: Structural + Lipinski (combinaison des conditions 2 et 3)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Températures spécifiques demandées\n",
    "    temperatures = [0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\n🧪 Test de {len(temperatures)} températures spécifiques\")\n",
    "    print(f\"Températures: {temperatures}\")\n",
    "    print(\"Génération de 1000 molécules par température...\")\n",
    "    print(\"Métriques évaluées: Validité, Novelty, Unicité, IntDiv\")\n",
    "\n",
    "    # Vérifier le fichier de référence d'abord\n",
    "    reference_path = os.path.join(DRIVE_PATH, DATA_FILE)\n",
    "    if not os.path.exists(reference_path):\n",
    "        print(f\"❌ ATTENTION: Fichier de référence non trouvé: {reference_path}\")\n",
    "        print(\"⚠️  La nouveauté sera calculée comme 100% (toutes les molécules considérées comme nouvelles)\")\n",
    "    else:\n",
    "        print(f\"✓ Fichier de référence trouvé: {DATA_FILE}\")\n",
    "\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n📊 Température: {temp}\")\n",
    "        result = evaluate_temperature_performance(\n",
    "            model, condition_tensor, stoi, itos, start_idx, end_idx, temp, 1000\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "        print(f\"   ✓ Validité: {result['validity']:.1f}% ({result['valid_count']}/1000)\")\n",
    "        print(f\"   ✓ Novelty: {result['novelty']:.1f}% ({result['novel_count']}/1000)\")\n",
    "        print(f\"   ✓ Unicité: {result['uniqueness']:.1f}% ({result['unique_count']}/1000)\")\n",
    "        print(f\"   ✓ IntDiv: {result['intdiv']:.3f}\")\n",
    "        print(f\"     Score de compromis: {result['balanced_score']:.1f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def display_comprehensive_results(results):\n",
    "    \"\"\"Affiche un tableau complet des résultats\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"📊 TABLEAU COMPLET DES RÉSULTATS - CONDITION 4\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Temp':<6} {'Validité':<10} {'Novelty':<10} {'Unicité':<10} {'IntDiv':<10} {'Compromis':<12} {'Valides':<8} {'Nouvelles':<9} {'Uniques':<8}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"{result['temperature']:<6} {result['validity']:<10.1f} {result['novelty']:<10.1f} \"\n",
    "              f\"{result['uniqueness']:<10.1f} {result['intdiv']:<10.3f} {result['balanced_score']:<12.1f} \"\n",
    "              f\"{result['valid_count']:<8} {result['novel_count']:<9} {result['unique_count']:<8}\")\n",
    "\n",
    "def find_optimal_temperature(results):\n",
    "    \"\"\"Trouve la température optimale basée sur le score de compromis\"\"\"\n",
    "    if not results:\n",
    "        return 0.5\n",
    "\n",
    "    # Trouver le résultat avec le meilleur score de compromis\n",
    "    best_result = max(results, key=lambda x: x['balanced_score'])\n",
    "\n",
    "    print(f\"\\n🎯 TEMPÉRATURE OPTIMALE TROUVÉE:\")\n",
    "    print(f\"   Température: {best_result['temperature']}\")\n",
    "    print(f\"   Score de compromis: {best_result['balanced_score']:.1f}\")\n",
    "    print(f\"   Validité: {best_result['validity']:.1f}%\")\n",
    "    print(f\"   Novelty: {best_result['novelty']:.1f}%\")\n",
    "    print(f\"   Unicité: {best_result['uniqueness']:.1f}%\")\n",
    "    print(f\"   IntDiv: {best_result['intdiv']:.3f}\")\n",
    "\n",
    "    return best_result['temperature']\n",
    "\n",
    "def plot_temperature_analysis(results):\n",
    "    \"\"\"Crée des graphiques pour visualiser les résultats\"\"\"\n",
    "    if not results:\n",
    "        print(\" Aucune donnée à visualiser\")\n",
    "        return\n",
    "\n",
    "    # Organiser les données\n",
    "    temperatures = [r['temperature'] for r in results]\n",
    "    validity = [r['validity'] for r in results]\n",
    "    novelty = [r['novelty'] for r in results]\n",
    "    uniqueness = [r['uniqueness'] for r in results]\n",
    "    intdiv = [r['intdiv'] * 100 for r in results]  # Normaliser pour le graphique\n",
    "    balanced_score = [r['balanced_score'] for r in results]\n",
    "\n",
    "    # Créer les graphiques\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Analyse de Sensibilité - Température (Condition 4)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Graphique 1: Validité\n",
    "    axes[0, 0].plot(temperatures, validity, marker='o', linewidth=2, color='blue')\n",
    "    axes[0, 0].set_xlabel('Température')\n",
    "    axes[0, 0].set_ylabel('Validité (%)')\n",
    "    axes[0, 0].set_title('Validité vs Température')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Graphique 2: Novelty\n",
    "    axes[0, 1].plot(temperatures, novelty, marker='o', linewidth=2, color='red')\n",
    "    axes[0, 1].set_xlabel('Température')\n",
    "    axes[0, 1].set_ylabel('Novelty (%)')\n",
    "    axes[0, 1].set_title('Novelty vs Température')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Graphique 3: Unicité\n",
    "    axes[1, 0].plot(temperatures, uniqueness, marker='o', linewidth=2, color='green')\n",
    "    axes[1, 0].set_xlabel('Température')\n",
    "    axes[1, 0].set_ylabel('Unicité (%)')\n",
    "    axes[1, 0].set_title('Unicité vs Température')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Graphique 4: Score de compromis\n",
    "    axes[1, 1].plot(temperatures, balanced_score, marker='o', linewidth=2, color='orange')\n",
    "    axes[1, 1].set_xlabel('Température')\n",
    "    axes[1, 1].set_ylabel('Score de Compromis')\n",
    "    axes[1, 1].set_title('Score de Compromis vs Température')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Sauvegarder le graphique\n",
    "    output_path = os.path.join(DRIVE_PATH, \"temperature_sensitivity_condition4.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Graphique sauvegardé: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# --- MAIN ---\n",
    "def main():\n",
    "    print(\" ANALYSE DE SENSIBILITÉ - TEMPÉRATURE OPTIMALE (CONDITION 4)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Objectif: Trouver la température qui offre le meilleur COMPROMIS entre:\")\n",
    "    print(\"  • Validité  • Novelty  • Unicité  • IntDiv\")\n",
    "    print(\"Condition: Structural + Lipinski\")\n",
    "    print(\"  - LogP≤3, MW≤480, HBA≤3, HBD≤3, RotB≤3 (Lipinski Ro3)\")\n",
    "    print(\"  - 2 aromatic rings, 1 non-aromatic, functional groups, R-value [0.05-0.50]\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Chargement vocabulaire\n",
    "    print(\"\\n📚 Chargement du vocabulaire...\")\n",
    "    with open(VOCAB_FILE, 'r', encoding='utf-8') as f:\n",
    "        vocab_data = json.load(f)\n",
    "    stoi = vocab_data['stoi']\n",
    "    itos = vocab_data['itos']\n",
    "\n",
    "    start_token = stoi['<start>']\n",
    "    end_token = stoi['<end>']\n",
    "    print(f\"✓ Vocabulaire chargé (Tokens: Start={start_token}, End={end_token})\")\n",
    "\n",
    "    # Chargement modèle\n",
    "    print(\"\\n🤖 Chargement du modèle...\")\n",
    "    config = GPTConfig(vocab_size=len(stoi))\n",
    "    model = ConditionalDrugGPT(config)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    print(\"✓ Modèle chargé et prêt\")\n",
    "\n",
    "    # Condition 4 pour la génération (Structural + Lipinski)\n",
    "    condition_vector = CONDITIONS[4][\"get_vector\"](None)   \n",
    "    condition_tensor = torch.tensor([condition_vector], dtype=torch.float32)\n",
    "\n",
    "    print(f\"\\n🎯 Condition de génération: {CONDITIONS[4]['name']}\")\n",
    "    print(f\"📝 Description: {CONDITIONS[4]['description']}\")\n",
    "\n",
    "    # Analyse de sensibilité\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"🎯 PHASE 1 : ANALYSE DE SENSIBILITÉ\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    results = temperature_sensitivity_analysis(\n",
    "        model, condition_tensor, stoi, itos, start_token, end_token\n",
    "    )\n",
    "\n",
    "    # Affichage du tableau complet\n",
    "    display_comprehensive_results(results)\n",
    "\n",
    "    # Trouver la température optimale\n",
    "    optimal_temp = find_optimal_temperature(results)\n",
    "\n",
    "    # Générer le graphique\n",
    "    plot_temperature_analysis(results)\n",
    "\n",
    "    # Sauvegarde des résultats\n",
    "    output_file = os.path.join(DRIVE_PATH, \"temperature_optimization_condition4_compromise.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"ANALYSE DE SENSIBILITÉ - TEMPÉRATURE OPTIMALE (CONDITION 4 - COMPROMIS)\\n\")\n",
    "        f.write(\"=\"*100 + \"\\n\\n\")\n",
    "        f.write(f\"Condition: {CONDITIONS[4]['name']}\\n\")\n",
    "        f.write(f\"Description: {CONDITIONS[4]['description']}\\n\\n\")\n",
    "        f.write(f\"Température optimale trouvée: {optimal_temp}\\n\")\n",
    "        f.write(f\"Score de compromis: {max(results, key=lambda x: x['balanced_score'])['balanced_score']:.1f}\\n\\n\")\n",
    "\n",
    "        f.write(\"RÉSULTATS DÉTAILLÉS:\\n\")\n",
    "        f.write(\"-\"*100 + \"\\n\")\n",
    "        f.write(f\"{'Temp':<6} {'Validité':<10} {'Novelty':<10} {'Unicité':<10} {'IntDiv':<10} {'Compromis':<12} {'Valides':<8} {'Nouvelles':<9} {'Uniques':<8}\\n\")\n",
    "        f.write(\"-\"*100 + \"\\n\")\n",
    "\n",
    "        for result in results:\n",
    "            f.write(f\"{result['temperature']:<6} {result['validity']:<10.1f} {result['novelty']:<10.1f} \"\n",
    "                   f\"{result['uniqueness']:<10.1f} {result['intdiv']:<10.3f} {result['balanced_score']:<12.1f} \"\n",
    "                   f\"{result['valid_count']:<8} {result['novel_count']:<9} {result['unique_count']:<8}\\n\")\n",
    "\n",
    "    print(f\"\\n💾 Résultats sauvegardés: {output_file}\")\n",
    "\n",
    "    print(\"\\n✅ ANALYSE TERMINÉE !\")\n",
    "    print(\"🎯 La température optimale a été trouvée pour la Condition 4\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1b11a",
   "metadata": {},
   "source": [
    "Condition satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86344a23",
   "metadata": {},
   "source": [
    "you can specify the condition of the generation here condition_vector = CONDITIONS[2][\"get_vector\"](None)\n",
    "</br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "SCRIPT : ANALYSE HIÉRARCHIQUE COMPLÈTE\n",
    "Valide → Novel → Unique → Conditions sur Uniques\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, DataStructs, Crippen, Lipinski\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DRIVE_PATH = '/content/drive/MyDrive/cond_gpt_model1'\n",
    "VOCAB_FILE = os.path.join(DRIVE_PATH, 'vocab_dataset.json')\n",
    "CHECKPOINT_FILE = os.path.join(DRIVE_PATH, 'checkpoints', 'cond_gpt_categorical_extended.pth')\n",
    "DATA_FILE = '/content/drive/MyDrive/cond_gpt_model1/s_100_str_+1M_fixed.txt'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- FONCTIONS IDENTIQUES ---\n",
    "def has_functional_group(mol):\n",
    "    smarts_patterns = [\n",
    "        '[OH]',\n",
    "        '[#6]C(=O)[O;H0]',\n",
    "        'C(=O)[OH]',\n",
    "        '[NH2]'\n",
    "    ]\n",
    "    for pattern in smarts_patterns:\n",
    "        if mol.HasSubstructMatch(Chem.MolFromSmarts(pattern)):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def calculate_r_value(mol):\n",
    "    try:\n",
    "        mol_wt = Descriptors.MolWt(mol)\n",
    "        logp = Crippen.MolLogP(mol)\n",
    "        if mol_wt > 0:\n",
    "            r_value = logp / (mol_wt / 100)\n",
    "            return r_value\n",
    "        else:\n",
    "            return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# --- DÉFINITION DES CONDITIONS ---\n",
    "CONDITIONS = {\n",
    "    0: {\n",
    "        \"name\": \"Condition 1: LogP ≤ 3\",\n",
    "        \"description\": \"Single objective: logP ≤ 3\",\n",
    "        \"get_vector\": lambda mol: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    },\n",
    "    1: {\n",
    "        \"name\": \"Condition 2: Structural\",\n",
    "        \"description\": \"2 aromatic rings, 1 non-aromatic, functional groups, R-value [0.05-0.50]\",\n",
    "        \"get_vector\": lambda mol: [0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0]\n",
    "    },\n",
    "    2: {\n",
    "        \"name\": \"Condition 3: Lipinski Ro3\",\n",
    "        \"description\": \"LogP≤3, MW≤480, HBA≤3, HBD≤3, RotB≤3\",\n",
    "        \"get_vector\": lambda mol: [1.0, 1.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "    },\n",
    "    3: {\n",
    "        \"name\": \"Condition 4: Structural + Lipinski\",\n",
    "        \"description\": \"Combination of conditions 2 and 3\",\n",
    "        \"get_vector\": lambda mol: [1.0, 1.0, 3.0, 3.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "def evaluate_condition(mol, condition_idx):\n",
    "    \"\"\"Évalue si une molécule satisfait une condition spécifique\"\"\"\n",
    "    try:\n",
    "        if condition_idx == 0:  # LogP ≤ 3\n",
    "            logp = Crippen.MolLogP(mol)\n",
    "            return logp <= 3.0\n",
    "\n",
    "        elif condition_idx == 1:  # Structural Objectives\n",
    "            aromatic_rings = Lipinski.NumAromaticRings(mol)\n",
    "            non_aromatic_rings = Lipinski.NumAliphaticRings(mol)\n",
    "            ring_condition = (aromatic_rings == 2) and (non_aromatic_rings == 1)\n",
    "            functional_group_condition = has_functional_group(mol)\n",
    "            r_value = calculate_r_value(mol)\n",
    "            r_value_condition = (0.05 <= r_value <= 0.50)\n",
    "            return ring_condition and functional_group_condition and r_value_condition\n",
    "\n",
    "        elif condition_idx == 2:  # Lipinski's Rule of Three\n",
    "            logp = Crippen.MolLogP(mol)\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            hbd = Lipinski.NumHDonors(mol)\n",
    "            hba = Lipinski.NumHAcceptors(mol)\n",
    "            rotb = Lipinski.NumRotatableBonds(mol)\n",
    "            return (logp <= 3.0) and (mw <= 480) and (hbd <= 3) and (hba <= 3) and (rotb <= 3)\n",
    "\n",
    "        elif condition_idx == 3:  # Structural + Lipinski\n",
    "            aromatic_rings = Lipinski.NumAromaticRings(mol)\n",
    "            non_aromatic_rings = Lipinski.NumAliphaticRings(mol)\n",
    "            ring_condition = (aromatic_rings == 2) and (non_aromatic_rings == 1)\n",
    "            functional_group_condition = has_functional_group(mol)\n",
    "            r_value = calculate_r_value(mol)\n",
    "            r_value_condition = (0.05 <= r_value <= 0.50)\n",
    "            logp = Crippen.MolLogP(mol)\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            hbd = Lipinski.NumHDonors(mol)\n",
    "            hba = Lipinski.NumHAcceptors(mol)\n",
    "            rotb = Lipinski.NumRotatableBonds(mol)\n",
    "            lipinski_condition = (logp <= 3.0) and (mw <= 480) and (hbd <= 3) and (hba <= 3) and (rotb <= 3)\n",
    "            return ring_condition and functional_group_condition and r_value_condition and lipinski_condition\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# --- ARCHITECTURE MODÈLE ---\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = 57\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.1\n",
    "    condition_dim: int = 10\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class ConditionalDrugGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.condition_projector = nn.Sequential(\n",
    "            nn.Linear(config.condition_dim, config.n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, conditions=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Séquence trop longue: {T}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        assert conditions is not None, \"Les conditions doivent être fournies !\"\n",
    "        cond_emb = self.condition_projector(conditions)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb + cond_emb.unsqueeze(1))\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "# --- ANALYSE HIÉRARCHIQUE COMPLÈTE ---\n",
    "def comprehensive_hierarchical_analysis(generated_smiles, reference_smiles_file):\n",
    "    \"\"\"Analyse hiérarchique complète : Valid → Novel → Unique → Conditions\"\"\"\n",
    "    print(\"🔍 ANALYSE HIÉRARCHIQUE COMPLÈTE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Charger le dataset de référence\n",
    "    reference_smiles = set()\n",
    "    if os.path.exists(reference_smiles_file):\n",
    "        print(\"📖 Chargement du dataset de référence...\")\n",
    "        with open(reference_smiles_file, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Chargement référence\"):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    mol = Chem.MolFromSmiles(line)\n",
    "                    if mol:\n",
    "                        canon_smiles = Chem.MolToSmiles(mol)\n",
    "                        reference_smiles.add(canon_smiles)\n",
    "        print(f\"✓ Référence chargée: {len(reference_smiles)} molécules\")\n",
    "\n",
    "    # Niveau 1: Validation\n",
    "    print(f\"\\n🎯 NIVEAU 1: VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    valid_molecules = []\n",
    "    valid_smiles = []\n",
    "\n",
    "    for smiles in tqdm(generated_smiles, desc=\"Validation SMILES\"):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            valid_molecules.append(mol)\n",
    "            valid_smiles.append(smiles)\n",
    "\n",
    "    total_generated = len(generated_smiles)\n",
    "    total_valid = len(valid_molecules)\n",
    "    validity_percentage = (total_valid / total_generated) * 100\n",
    "\n",
    "    print(f\"• Générées: {total_generated}\")\n",
    "    print(f\"• Valides: {total_valid} ({validity_percentage:.1f}%)\")\n",
    "\n",
    "    # Niveau 2: Novelty\n",
    "    print(f\"\\n🎯 NIVEAU 2: NOVELTY\")\n",
    "    print(\"-\" * 40)\n",
    "    novel_molecules = []\n",
    "    novel_smiles = []\n",
    "\n",
    "    for mol, smiles in tqdm(zip(valid_molecules, valid_smiles), desc=\"Vérification novelty\", total=total_valid):\n",
    "        canon_smiles = Chem.MolToSmiles(mol)\n",
    "        if canon_smiles not in reference_smiles:\n",
    "            novel_molecules.append(mol)\n",
    "            novel_smiles.append(smiles)\n",
    "\n",
    "    total_novel = len(novel_molecules)\n",
    "    novelty_percentage = (total_novel / total_valid) * 100\n",
    "\n",
    "    print(f\"• Valides: {total_valid}\")\n",
    "    print(f\"• Novel: {total_novel} ({novelty_percentage:.1f}%)\")\n",
    "\n",
    "    # Niveau 3: Uniqueness parmi les Novel\n",
    "    print(f\"\\n🎯 NIVEAU 3: UNICITÉ (parmi les Novel)\")\n",
    "    print(\"-\" * 40)\n",
    "    unique_novel_molecules = {}\n",
    "    unique_novel_smiles = []\n",
    "\n",
    "    for mol, smiles in tqdm(zip(novel_molecules, novel_smiles), desc=\"Déduplication novel\", total=total_novel):\n",
    "        canon_smiles = Chem.MolToSmiles(mol)\n",
    "        if canon_smiles not in unique_novel_molecules:\n",
    "            unique_novel_molecules[canon_smiles] = mol\n",
    "            unique_novel_smiles.append(smiles)\n",
    "\n",
    "    total_unique_novel = len(unique_novel_molecules)\n",
    "    uniqueness_percentage = (total_unique_novel / total_novel) * 100\n",
    "\n",
    "    print(f\"• Novel: {total_novel}\")\n",
    "    print(f\"• Uniques parmi novel: {total_unique_novel} ({uniqueness_percentage:.1f}%)\")\n",
    "\n",
    "    # Niveau 4: Satisfaction des conditions sur les Uniques Novel\n",
    "    print(f\"\\n🎯 NIVEAU 4: SATISFACTION DES CONDITIONS (sur Uniques Novel)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    condition_results = {\n",
    "        'total_unique_novel': total_unique_novel,\n",
    "        'condition_counts': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "        'condition_percentages': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
    "        'examples_per_condition': {0: [], 1: [], 2: [], 3: []},\n",
    "        'properties_per_condition': {0: [], 1: [], 2: [], 3: []}\n",
    "    }\n",
    "\n",
    "    for condition_idx in range(4):\n",
    "        print(f\"\\n🔍 {CONDITIONS[condition_idx]['name']}\")\n",
    "        condition_count = 0\n",
    "\n",
    "        for canon_smiles, mol in tqdm(unique_novel_molecules.items(),\n",
    "                                    desc=f\"Condition {condition_idx+1}\",\n",
    "                                    leave=False):\n",
    "            if evaluate_condition(mol, condition_idx):\n",
    "                condition_count += 1\n",
    "\n",
    "                # Garder des exemples\n",
    "                if len(condition_results['examples_per_condition'][condition_idx]) < 3:\n",
    "                    condition_results['examples_per_condition'][condition_idx].append(canon_smiles)\n",
    "\n",
    "                    # Propriétés\n",
    "                    props = {\n",
    "                        'LogP': Crippen.MolLogP(mol),\n",
    "                        'MW': Descriptors.MolWt(mol),\n",
    "                        'HBD': Lipinski.NumHDonors(mol),\n",
    "                        'HBA': Lipinski.NumHAcceptors(mol),\n",
    "                        'RotB': Lipinski.NumRotatableBonds(mol),\n",
    "                    }\n",
    "                    if condition_idx in [1, 3]:  # Conditions structurelles\n",
    "                        props.update({\n",
    "                            'Aromatic': Lipinski.NumAromaticRings(mol),\n",
    "                            'Aliphatic': Lipinski.NumAliphaticRings(mol),\n",
    "                            'R-value': calculate_r_value(mol),\n",
    "                            'Functional': has_functional_group(mol)\n",
    "                        })\n",
    "                    condition_results['properties_per_condition'][condition_idx].append(props)\n",
    "\n",
    "        condition_results['condition_counts'][condition_idx] = condition_count\n",
    "        if total_unique_novel > 0:\n",
    "            condition_results['condition_percentages'][condition_idx] = (condition_count / total_unique_novel) * 100\n",
    "\n",
    "        print(f\"   ✓ Satisfait: {condition_count}/{total_unique_novel} ({condition_results['condition_percentages'][condition_idx]:.1f}%)\")\n",
    "\n",
    "    # Résumé complet\n",
    "    results = {\n",
    "        'total_generated': total_generated,\n",
    "        'total_valid': total_valid,\n",
    "        'validity_percentage': validity_percentage,\n",
    "        'total_novel': total_novel,\n",
    "        'novelty_percentage': novelty_percentage,\n",
    "        'total_unique_novel': total_unique_novel,\n",
    "        'uniqueness_percentage': uniqueness_percentage,\n",
    "        'condition_results': condition_results,\n",
    "        'unique_novel_molecules': unique_novel_molecules\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def display_hierarchical_results(results):\n",
    "    \"\"\"Affiche les résultats de l'analyse hiérarchique\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"📊 RÉSULTATS HIÉRARCHIQUES COMPLETS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\n🎯 HIÉRARCHIE DE QUALITÉ:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"1. Générées:      {results['total_generated']:>6} molécules (100.0%)\")\n",
    "    print(f\"2. Valides:       {results['total_valid']:>6} molécules ({results['validity_percentage']:5.1f}% des générées)\")\n",
    "    print(f\"3. Novel:         {results['total_novel']:>6} molécules ({results['novelty_percentage']:5.1f}% des valides)\")\n",
    "    print(f\"4. Uniques Novel: {results['total_unique_novel']:>6} molécules ({results['uniqueness_percentage']:5.1f}% des novel)\")\n",
    "\n",
    "    print(f\"\\n🎯 SATISFACTION DES CONDITIONS (sur {results['total_unique_novel']} uniques novel):\")\n",
    "    print(\"-\" * 50)\n",
    "    for condition_idx in range(4):\n",
    "        count = results['condition_results']['condition_counts'][condition_idx]\n",
    "        percentage = results['condition_results']['condition_percentages'][condition_idx]\n",
    "        print(f\"• {CONDITIONS[condition_idx]['name']}:\")\n",
    "        print(f\"     {count:>3} molécules ({percentage:5.1f}% des uniques novel)\")\n",
    "\n",
    "    # Score final\n",
    "    if results['total_unique_novel'] > 0:\n",
    "        final_score = (results['total_unique_novel'] / results['total_generated']) * 100\n",
    "        print(f\"\\n⚖️  SCORE FINAL: {final_score:.2f}% des molécules générées sont UNIQUES, NOUVELLES et VALIDES\")\n",
    "\n",
    "def calculate_intdiv(smiles_list):\n",
    "    \"\"\"Calcule la diversité interne sur un échantillon\"\"\"\n",
    "    print(\"🔍 Calcul de la diversité interne (IntDiv)...\")\n",
    "\n",
    "    if len(smiles_list) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Échantillonnage pour performance\n",
    "    sample_size = min(1000, len(smiles_list))\n",
    "    if len(smiles_list) > 1000:\n",
    "        indices = np.random.choice(len(smiles_list), sample_size, replace=False)\n",
    "        sample_smiles = [smiles_list[i] for i in indices]\n",
    "    else:\n",
    "        sample_smiles = smiles_list\n",
    "\n",
    "    fingerprints = []\n",
    "    for smiles in tqdm(sample_smiles, desc=\"Fingerprints\"):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "            fingerprints.append(fp)\n",
    "\n",
    "    if len(fingerprints) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    similarities = []\n",
    "    for i in tqdm(range(len(fingerprints)), desc=\"Similarités\"):\n",
    "        for j in range(i+1, len(fingerprints)):\n",
    "            similarity = DataStructs.TanimotoSimilarity(fingerprints[i], fingerprints[j])\n",
    "            similarities.append(similarity)\n",
    "\n",
    "    if similarities:\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        intdiv = 1 - mean_similarity\n",
    "        print(f\"✓ IntDiv: {intdiv:.4f} (sur {len(sample_smiles)} molécules)\")\n",
    "        return intdiv\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# --- GÉNÉRATION ---\n",
    "@torch.no_grad()\n",
    "def generate_molecules(model, condition_tensor, stoi, itos, start_idx, end_idx, num_molecules=10000, temperature=0.6):\n",
    "    \"\"\"Génère des molécules avec température spécifique\"\"\"\n",
    "    print(f\"🎯 Génération de {num_molecules} molécules (température: {temperature})\")\n",
    "\n",
    "    generated_smiles = []\n",
    "\n",
    "    with tqdm(total=num_molecules, desc=\"Génération\") as pbar:\n",
    "        while len(generated_smiles) < num_molecules:\n",
    "            top_k = 30\n",
    "            idx = torch.tensor([[start_idx]], dtype=torch.long, device=DEVICE)\n",
    "            condition_local = condition_tensor.to(DEVICE)\n",
    "\n",
    "            for step in range(80):\n",
    "                idx_cond = idx if idx.size(1) <= 128 else idx[:, -128:]\n",
    "                logits, _ = model(idx_cond, conditions=condition_local)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                probs_mod = probs.clone()\n",
    "                probs_mod[0, start_idx] = 0.0\n",
    "                if step < 8:\n",
    "                    probs_mod[0, end_idx] = 0.0\n",
    "\n",
    "                if probs_mod.sum() > 0:\n",
    "                    probs_mod = probs_mod / probs_mod.sum()\n",
    "                else:\n",
    "                    probs_mod = probs\n",
    "\n",
    "                idx_next = torch.multinomial(probs_mod, num_samples=1)\n",
    "                next_token = idx_next.item()\n",
    "\n",
    "                if next_token == end_idx and step >= 8:\n",
    "                    break\n",
    "\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "            tokens = idx[0].tolist()\n",
    "            if len(tokens) > 1:\n",
    "                tokens_to_decode = tokens[1:]\n",
    "                if end_idx in tokens_to_decode:\n",
    "                    end_pos = tokens_to_decode.index(end_idx)\n",
    "                    tokens_to_decode = tokens_to_decode[:end_pos]\n",
    "            else:\n",
    "                tokens_to_decode = []\n",
    "\n",
    "            smiles = ''.join([itos[str(i)] for i in tokens_to_decode if str(i) in itos])\n",
    "\n",
    "            if smiles:\n",
    "                generated_smiles.append(smiles)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"✅ {len(generated_smiles)} molécules générées\")\n",
    "    return generated_smiles\n",
    "\n",
    "# --- MAIN ---\n",
    "def main():\n",
    "    print(\"🚀 ANALYSE HIÉRARCHIQUE COMPLÈTE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Hiérarchie: Générées → Valides → Novel → Uniques Novel → Conditions\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Chargements\n",
    "    print(\"\\n📚 Chargement du vocabulaire...\")\n",
    "    with open(VOCAB_FILE, 'r', encoding='utf-8') as f:\n",
    "        vocab_data = json.load(f)\n",
    "    stoi = vocab_data['stoi']\n",
    "    itos = vocab_data['itos']\n",
    "    start_token = stoi['<start>']\n",
    "    end_token = stoi['<end>']\n",
    "\n",
    "    print(\"🤖 Chargement du modèle...\")\n",
    "    config = GPTConfig(vocab_size=len(stoi))\n",
    "    model = ConditionalDrugGPT(config)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Condition de génération\n",
    "    condition_vector = CONDITIONS[2][\"get_vector\"](None)\n",
    "    condition_tensor = torch.tensor([condition_vector], dtype=torch.float32)\n",
    "\n",
    "    print(f\"\\n🎯 Génération avec:\")\n",
    "    print(f\"• Température: 0.6\")\n",
    "    print(f\"• Condition: {CONDITIONS[2]['name']}\")\n",
    "    print(f\"• Molécules: 10,000\")\n",
    "\n",
    "    # Génération\n",
    "    generated_smiles = generate_molecules(\n",
    "        model, condition_tensor, stoi, itos, start_token, end_token,\n",
    "        num_molecules=10000, temperature=0.6\n",
    "    )\n",
    "\n",
    "    # Analyse hiérarchique\n",
    "    results = comprehensive_hierarchical_analysis(generated_smiles, DATA_FILE)\n",
    "\n",
    "    # Calcul IntDiv sur les uniques novel\n",
    "    unique_novel_smiles = list(results['unique_novel_molecules'].keys())\n",
    "    intdiv = calculate_intdiv(unique_novel_smiles)\n",
    "    results['intdiv'] = intdiv\n",
    "\n",
    "    # Affichage résultats\n",
    "    display_hierarchical_results(results)\n",
    "\n",
    "    print(f\"\\n📈 Diversité (IntDiv) des uniques novel: {intdiv:.4f}\")\n",
    "    print(\"\\n✅ ANALYSE TERMINÉE !\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
