{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61e49c5-6d42-4bec-aa5e-59863e1ee7aa",
   "metadata": {
    "papermill": {
     "duration": 0.006483,
     "end_time": "2025-10-21T19:59:10.706915",
     "exception": false,
     "start_time": "2025-10-21T19:59:10.700432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3526b052-3dfa-449b-840e-31f559559081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T19:59:10.716811Z",
     "iopub.status.busy": "2025-10-21T19:59:10.715860Z",
     "iopub.status.idle": "2025-10-21T19:59:11.507756Z",
     "shell.execute_reply": "2025-10-21T19:59:11.506226Z"
    },
    "papermill": {
     "duration": 0.798713,
     "end_time": "2025-10-21T19:59:11.509763",
     "exception": true,
     "start_time": "2025-10-21T19:59:10.711050",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BackendError",
     "evalue": "POST failed with: {\"errors\":[\"New Datasets cannot be attached in non-interactive sessions. Found no versions attached for Dataset [ouical/data-gen].\"],\"error\":{\"code\":9},\"wasSuccessful\":false}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/3090221043.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Download latest version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ouical/data-gen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Path to dataset files:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Dataset: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/kaggle_cache_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mdataset_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"VersionNumber\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion_from_package_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         result = client.post(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mATTACH_DATASOURCE_REQUEST_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             {\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, request_name, data, timeout)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wasSuccessful\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"POST failed with: {response.text!s}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBackendError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"result\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'result' field missing from response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBackendError\u001b[0m: POST failed with: {\"errors\":[\"New Datasets cannot be attached in non-interactive sessions. Found no versions attached for Dataset [ouical/data-gen].\"],\"error\":{\"code\":9},\"wasSuccessful\":false}"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ouical/data-gen\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb489a86-31d1-47d9-bd75-0dc8cf51dbd3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SCRIPT COMPLET AM√âLIOR√â : TRANSFORMATEUR CONDITIONNEL (Cat√©goriel) -\n",
    "\"\"\"\n",
    "\n",
    "# --- PARTIE 1 : IMPORTS ET CONFIGURATION ---\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from rdkit import rdBase\n",
    "    rdBase.DisableLog('rdApp.error')\n",
    "except ImportError:\n",
    "    print(\"Erreur : RDKit n'est pas install√©.\")\n",
    "    print(\"Veuillez l'installer avec : pip install rdkit\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725477d-8917-4d0f-bd29-2b87b4edb86b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration AM√âLIOR√âE ---\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 128\n",
    "MAX_ITERS = 15000  # Augment√©\n",
    "EVAL_INTERVAL = 500\n",
    "LEARNING_RATE = 1e-4  # R√©duit\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EVAL_ITERS = 200\n",
    "N_EMBD = 256  # Augment√©\n",
    "N_HEAD = 8    # Augment√©\n",
    "N_LAYER = 8   # Augment√©\n",
    "DROPOUT = 0.1\n",
    "CONDITION_DIM = 3\n",
    "\n",
    "# Fichiers\n",
    "DATA_FILE = 's_100_str_1M_fixed.txt'\n",
    "VOCAB_FILE = 'vocab_dataset.json'\n",
    "DATA_CACHE_FILE = 'data_cache_categorical_improved.pt'\n",
    "\n",
    "# Checkpoints\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, 'cond_gpt_categorical_improved.pth')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration : P√©riph√©rique={DEVICE}, Batch={BATCH_SIZE}, Contexte={BLOCK_SIZE}\")\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135268e-0e03-4980-9f1c-369c89ad22c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------ UTILITAIRE LECTURE SMILES ------------------\n",
    "def yield_smiles_from_file(filepath):\n",
    "    \"\"\"Rend chaque SMILES du fichier\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('<') and '><' in line:\n",
    "                for smi in line.strip().strip('<>').split('><'):\n",
    "                    smi = smi.strip()\n",
    "                    if smi:\n",
    "                        yield smi\n",
    "            else:\n",
    "                yield line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a1e4c-a267-45aa-b1cd-ae5a5f3e2751",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 2 : CONSTRUCTION DU VOCABULAIRE AM√âLIOR√âE ---\n",
    "\n",
    "print(f\"Construction du vocabulaire √† partir de '{DATA_FILE}'...\")\n",
    "\n",
    "PAD_TOKEN = '<pad>'\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "\n",
    "# Construction du vocabulaire avec normalisation\n",
    "char_set = set()\n",
    "n_seen = 0\n",
    "valid_smiles_count = 0\n",
    "\n",
    "for smiles in yield_smiles_from_file(DATA_FILE):\n",
    "    # Pr√©-filtrage avec RDKit\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        try:\n",
    "            canon_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "            char_set.update(list(canon_smiles))\n",
    "            valid_smiles_count += 1\n",
    "        except:\n",
    "            continue\n",
    "    n_seen += 1\n",
    "\n",
    "# Vocabulaire\n",
    "special_tokens = [PAD_TOKEN, START_TOKEN, END_TOKEN]\n",
    "vocabulary = special_tokens + sorted(set(char_set))\n",
    "\n",
    "# Dictionnaires\n",
    "stoi = { ch:i for i,ch in enumerate(vocabulary) }\n",
    "itos = { i:ch for i,ch in enumerate(vocabulary) }\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Sauvegarde\n",
    "with open(VOCAB_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump({'stoi': stoi, 'itos': itos}, f, indent=2)\n",
    "\n",
    "print(f\"Taille vocabulaire : {vocab_size} | SMILES valides: {valid_smiles_count}/{n_seen}\")\n",
    "\n",
    "# Fonctions encode/decode\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "decode = lambda l: ''.join([itos[i] for i in l if i in itos])\n",
    "\n",
    "# Test\n",
    "print(\"\\n=== TEST VOCABULAIRE ===\")\n",
    "test_smiles = \"CCO\"\n",
    "encoded = encode(test_smiles)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Test: {test_smiles} -> Encoded: {encoded} -> Decoded: {decoded} -> Match: {test_smiles == decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d17f24-ac4b-46b4-bf9a-1b0abb138112",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 3 : PR√âPARATION DES DONN√âES AM√âLIOR√âE ---\n",
    "\n",
    "LOGP_BINS = [0.0, 3.0, 5.0]\n",
    "MW_BINS   = [250.0, 480.0, 650.0]\n",
    "HBD_BINS  = [0.0, 1.0, 2.0, 3.0]\n",
    "\n",
    "def get_category(value, bins):\n",
    "    for i, upper_bound in enumerate(bins):\n",
    "        if value <= upper_bound:\n",
    "            return float(i)\n",
    "    return float(len(bins))\n",
    "\n",
    "def load_and_process_data_improved(filepath, stoi, max_len=BLOCK_SIZE, cache_file=DATA_CACHE_FILE):\n",
    "    \"\"\"Version am√©lior√©e avec normalisation des SMILES\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Chargement des donn√©es depuis le cache '{cache_file}'...\")\n",
    "        data = torch.load(cache_file)\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            print(f\"Donn√©es charg√©es ({len(data)} exemples).\")\n",
    "            return data\n",
    "        else:\n",
    "            print(\"Cache vide/corrompu, re-traitement...\")\n",
    "\n",
    "    print(\"Traitement des donn√©es SMILES avec normalisation...\")\n",
    "    data_processed = []\n",
    "    pad_idx = stoi[PAD_TOKEN]\n",
    "    start_idx = stoi[START_TOKEN]\n",
    "    end_idx = stoi[END_TOKEN]\n",
    "\n",
    "    try:\n",
    "        for i, smiles in enumerate(tqdm(yield_smiles_from_file(filepath), desc=\"Traitement\")):\n",
    "            # V√©rification RDKit\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Normalisation\n",
    "                canon_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "                \n",
    "                # V√©rification longueur et caract√®res\n",
    "                if len(canon_smiles) > max_len - 2 or not all(c in stoi for c in canon_smiles):\n",
    "                    continue\n",
    "\n",
    "                # Calcul propri√©t√©s\n",
    "                logp = Descriptors.MolLogP(mol)\n",
    "                mw   = Descriptors.MolWt(mol)\n",
    "                hbd  = Descriptors.NumHDonors(mol)\n",
    "\n",
    "                logp_cat = get_category(logp, LOGP_BINS)\n",
    "                mw_cat   = get_category(mw, MW_BINS)\n",
    "                hbd_cat  = get_category(hbd, HBD_BINS)\n",
    "\n",
    "                condition_vector = torch.tensor([logp_cat, mw_cat, hbd_cat], dtype=torch.float32)\n",
    "\n",
    "                # Encodage\n",
    "                token_ids = [start_idx] + encode(canon_smiles) + [end_idx]\n",
    "                seq_len = len(token_ids)\n",
    "                \n",
    "                x = torch.full((max_len,), pad_idx, dtype=torch.long)\n",
    "                y = torch.full((max_len,), pad_idx, dtype=torch.long)\n",
    "                x[:seq_len] = torch.tensor(token_ids, dtype=torch.long)\n",
    "                y[:seq_len-1] = torch.tensor(token_ids[1:], dtype=torch.long)\n",
    "\n",
    "                data_processed.append((x, y, condition_vector))\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            if i > 0 and i % 50000 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERREUR: Fichier '{filepath}' introuvable.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"\\nNombre total de mol√©cules valides : {len(data_processed)}\")\n",
    "    \n",
    "    if len(data_processed) == 0:\n",
    "        print(\"‚ö†Ô∏è Aucun exemple valide.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Sauvegarde dans le cache '{cache_file}'...\")\n",
    "    torch.save(data_processed, cache_file)\n",
    "    return data_processed\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42b7f3-ff23-467c-9ad4-f5f589099413",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 4 : ARCHITECTURE DU MOD√àLE (INCHANG√âE MAIS AVEC CONFIG AM√âLIOR√âE) ---\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    vocab_size: int = vocab_size\n",
    "    n_layer: int = N_LAYER\n",
    "    n_head: int = N_HEAD\n",
    "    n_embd: int = N_EMBD\n",
    "    dropout: float = DROPOUT\n",
    "    condition_dim: int = CONDITION_DIM\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class ConditionalDrugGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "\n",
    "        self.condition_projector = nn.Sequential(\n",
    "            nn.Linear(config.condition_dim, config.n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, conditions=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"S√©quence trop longue: {T}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "\n",
    "        assert conditions is not None, \"Conditions requises !\"\n",
    "        cond_emb = self.condition_projector(conditions)\n",
    "\n",
    "        x = self.transformer.drop(tok_emb + pos_emb + cond_emb.unsqueeze(1))\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=stoi[PAD_TOKEN]\n",
    "            )\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6093d-29f4-42e6-bd4c-6c1b0bb5e59a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 5 : FONCTIONS UTILITAIRES AM√âLIOR√âES ---\n",
    "\n",
    "def save_checkpoint(model, optimizer, iter_num, best_val_loss, config, filepath):\n",
    "    print(f\"Sauvegarde du checkpoint dans {filepath}...\")\n",
    "    torch.save({\n",
    "        'iter_num': iter_num,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': config,\n",
    "    }, filepath)\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"Aucun checkpoint trouv√©. D√©marrage d'un nouvel entra√Ænement.\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "    print(f\"Chargement du checkpoint depuis {filepath}...\")\n",
    "    checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Reprise √† l'it√©ration {iter_num} (meilleure perte val: {best_val_loss:.4f})\")\n",
    "    return iter_num, best_val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader, eval_iters=EVAL_ITERS):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        loader = train_loader if split == 'train' else val_loader\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        loader_iter = iter(loader)\n",
    "        for k in range(eval_iters):\n",
    "            try:\n",
    "                x, y, c = next(loader_iter)\n",
    "            except StopIteration:\n",
    "                loader_iter = iter(loader)\n",
    "                x, y, c = next(loader_iter)\n",
    "\n",
    "            x, y, c = x.to(DEVICE), y.to(DEVICE), c.to(DEVICE)\n",
    "            logits, loss = model(x, y, c)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21db8d-ecf8-4654-9a9f-c08f1676f637",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 6 : G√âN√âRATION AM√âLIOR√âE ---\n",
    "\n",
    "def validate_and_repair_smiles(smiles):\n",
    "    \"\"\"Tente de r√©parer les SMILES invalides\"\"\"\n",
    "    if not smiles or smiles == \"[VIDE]\":\n",
    "        return None\n",
    "        \n",
    "    # Essayer directement\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    \n",
    "    # Tentatives de r√©paration\n",
    "    try:\n",
    "        # Fermer les parenth√®ses\n",
    "        open_count = smiles.count('(')\n",
    "        close_count = smiles.count(')')\n",
    "        if open_count > close_count:\n",
    "            smiles += ')' * (open_count - close_count)\n",
    "        \n",
    "        # Fermer les cycles\n",
    "        ring_numbers = set()\n",
    "        for char in smiles:\n",
    "            if char.isdigit():\n",
    "                ring_numbers.add(char)\n",
    "        \n",
    "        for ring_num in ring_numbers:\n",
    "            if smiles.count(ring_num) % 2 != 0:\n",
    "                smiles += ring_num\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_conditional_improved(model, condition_tensor, stoi, itos, max_new_tokens=80, temperature=0.8, top_k=12):\n",
    "    \"\"\"Version am√©lior√©e de la g√©n√©ration\"\"\"\n",
    "    model.eval()\n",
    "    start_idx = stoi[START_TOKEN]\n",
    "    end_idx = stoi[END_TOKEN]\n",
    "\n",
    "    idx = torch.tensor([[start_idx]], dtype=torch.long, device=DEVICE)\n",
    "    condition_tensor = condition_tensor.to(DEVICE)\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "        \n",
    "        logits, _ = model(idx_cond, conditions=condition_tensor)\n",
    "        logits = logits[:, -1, :] / max(temperature, 0.1)  # √âviter division par z√©ro\n",
    "        \n",
    "        # Top-k filtering\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # √âchantillonnage avec contrainte pour √©viter les tokens improbables\n",
    "        if torch.isnan(probs).any() or torch.max(probs) < 0.01:\n",
    "            idx_next = torch.tensor([[stoi['C']]], device=DEVICE)  # Fallback\n",
    "        else:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Arr√™t si token de fin\n",
    "        if idx_next.item() == end_idx:\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        # Arr√™t pr√©coce si s√©quence trop longue\n",
    "        if idx.size(1) >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    # D√©codage\n",
    "    generated_tokens = idx[0].tolist()\n",
    "    tokens_to_decode = generated_tokens[1:]  # Exclure <start>\n",
    "    \n",
    "    # Trouver le premier <end>\n",
    "    if end_idx in tokens_to_decode:\n",
    "        end_pos = tokens_to_decode.index(end_idx)\n",
    "        tokens_to_decode = tokens_to_decode[:end_pos]\n",
    "    \n",
    "    generated_smiles = decode(tokens_to_decode)\n",
    "    \n",
    "    return generated_smiles if generated_smiles else \"[VIDE]\"\n",
    "\n",
    "def generate_with_retry(model, condition_tensor, stoi, itos, max_retries=3):\n",
    "    \"\"\"G√©n√®re avec plusieurs tentatives\"\"\"\n",
    "    best_smiles = None\n",
    "    best_length = 0\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        # Ajuster la temp√©rature progressivement\n",
    "        temp = 0.7 + attempt * 0.2\n",
    "        top_k = 10 + attempt * 5\n",
    "        \n",
    "        smiles = generate_conditional_improved(\n",
    "            model, condition_tensor, stoi, itos, \n",
    "            temperature=temp,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        # Tenter r√©paration\n",
    "        repaired = validate_and_repair_smiles(smiles)\n",
    "        if repaired and Chem.MolFromSmiles(repaired):\n",
    "            # Pr√©f√©rer les SMILES de longueur raisonnable\n",
    "            if 10 <= len(repaired) <= 60:\n",
    "                return repaired\n",
    "            elif best_smiles is None or abs(len(repaired) - 35) < abs(best_length - 35):\n",
    "                best_smiles = repaired\n",
    "                best_length = len(repaired)\n",
    "    \n",
    "    return best_smiles if best_smiles else \"[VIDE]\"\n",
    "\n",
    "def check_mol_3_props(smiles):\n",
    "    \"\"\"V√©rifie les 3 propri√©t√©s r√©elles\"\"\"\n",
    "    if not smiles or smiles == \"[VIDE]\":\n",
    "        return \"Vide\", 0.0, 0.0, 0\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return \"Invalide\", 0.0, 0.0, 0\n",
    "    try:\n",
    "        logp = Descriptors.MolLogP(mol)\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        hbd = Descriptors.NumHDonors(mol)\n",
    "        return \"Valide\", logp, mw, hbd\n",
    "    except:\n",
    "        return \"Erreur\", 0.0, 0.0, 0\n",
    "\n",
    "def evaluate_generation_quality(generated_smiles, target_conditions):\n",
    "    \"\"\"√âvalue la qualit√© des g√©n√©rations\"\"\"\n",
    "    results = {\n",
    "        'valid': 0,\n",
    "        'matching_conditions': 0,\n",
    "        'unique': 0,\n",
    "        'avg_length': 0\n",
    "    }\n",
    "    \n",
    "    unique_smiles = set()\n",
    "    valid_smiles = []\n",
    "    \n",
    "    for smi in generated_smiles:\n",
    "        status, logp, mw, hbd = check_mol_3_props(smi)\n",
    "        \n",
    "        if status == \"Valide\":\n",
    "            results['valid'] += 1\n",
    "            unique_smiles.add(smi)\n",
    "            valid_smiles.append(smi)\n",
    "            results['avg_length'] += len(smi)\n",
    "            \n",
    "            # V√©rifier correspondance conditions\n",
    "            logp_cat = get_category(logp, LOGP_BINS)\n",
    "            mw_cat = get_category(mw, MW_BINS)\n",
    "            hbd_cat = get_category(hbd, HBD_BINS)\n",
    "            \n",
    "            if (logp_cat == target_conditions[0] and \n",
    "                mw_cat == target_conditions[1] and \n",
    "                hbd_cat == target_conditions[2]):\n",
    "                results['matching_conditions'] += 1\n",
    "    \n",
    "    results['unique'] = len(unique_smiles)\n",
    "    if results['valid'] > 0:\n",
    "        results['avg_length'] /= results['valid']\n",
    "    \n",
    "    return results, valid_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1b2f5-4792-44a9-8228-85b22216764b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 7 : SCRIPT PRINCIPAL AM√âLIOR√â ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1. Chargement des donn√©es am√©lior√©\n",
    "    full_data = load_and_process_data_improved(DATA_FILE, stoi, cache_file=DATA_CACHE_FILE)\n",
    "    if len(full_data) == 0:\n",
    "        print(\"Arr√™t : aucun √©chantillon valide.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. √âquilibrage des classes\n",
    "    print(\"\\n‚öñÔ∏è Calcul des poids d'√©chantillonnage...\")\n",
    "    combo_counts = Counter([tuple(sample[2].tolist()) for sample in full_data])\n",
    "    total_samples = len(full_data)\n",
    "    num_classes = len(combo_counts)\n",
    "\n",
    "    weights = []\n",
    "    for _, _, cond in full_data:\n",
    "        key = tuple(cond.tolist())\n",
    "        weights.append(total_samples / (num_classes * combo_counts[key]))\n",
    "\n",
    "    print(f\"‚úÖ Combinaisons de classes : {num_classes}\")\n",
    "    print(f\"Exemple de poids : {list(weights)[:5]}\")\n",
    "\n",
    "    # 3. Dataset et DataLoaders\n",
    "    dataset = SMILESDataset(full_data)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_indices = train_data.indices\n",
    "    train_weights = [weights[i] for i in train_indices]\n",
    "    sampler = WeightedRandomSampler(train_weights, num_samples=len(train_indices), replacement=True)\n",
    "\n",
    "    pin_mem = True if DEVICE == 'cuda' else False\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=sampler, \n",
    "                             num_workers=0, pin_memory=pin_mem)\n",
    "    val_loader   = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             num_workers=0, pin_memory=pin_mem)\n",
    "\n",
    "    print(\"‚úÖ √âchantillonnage √©quilibr√© activ√©.\")\n",
    "\n",
    "    # 4. Mod√®le avec configuration am√©lior√©e\n",
    "    config = GPTConfig(vocab_size=vocab_size)\n",
    "    model = ConditionalDrugGPT(config).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    \n",
    "    param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    print(f\"Nombre de param√®tres : {param_count:.2f} M\")\n",
    "\n",
    "    # 5. Checkpoint\n",
    "    start_iter, best_val_loss = load_checkpoint(CHECKPOINT_FILE, model, optimizer)\n",
    "\n",
    "    # 6. Entra√Ænement am√©lior√©\n",
    "    print(f\"üöÄ D√©but de l'entra√Ænement sur {DEVICE}...\")\n",
    "    start_time = time.time()\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    for iter_num in range(start_iter, MAX_ITERS):\n",
    "        try:\n",
    "            xb, yb, cb = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            xb, yb, cb = next(train_iter)\n",
    "\n",
    "        xb, yb, cb = xb.to(DEVICE), yb.to(DEVICE), cb.to(DEVICE)\n",
    "        logits, loss = model(xb, targets=yb, conditions=cb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter_num % 1000 == 0:\n",
    "    results3 = test_generation_complete(\"[LogP(2), MW(2), HBD(1)]\", [2.0, 2.0, 1.0])\n",
    "    \n",
    "    # R√©sum√© final\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà R√âSUM√â FINAL DES PERFORMANCES\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Validit√© moyenne: {(results1['valid'] + results2['valid'] + results3['valid']) / 90 * 100:.1f}%\")\n",
    "    print(f\"Correspondance moyenne: {(results1['matching_conditions'] + results2['matching_conditions'] + results3['matching_conditions']) / 90 * 100:.1f}%\")\n",
    "    print(f\"Unicit√© moyenne: {(results1['unique'] + results2['unique'] + results3['unique']) / 90 * 100:.1f}%\")\n",
    "\n",
    "    print(\"\\n‚úÖ Script termin√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbede6cc-5fce-4c18-8496-15bb90bc4f81",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---  Copier ici ton texte de log ---\n",
    "log_text = \"\"\"\n",
    "√âtape 500: train 1.2334, val 1.2766, temps 67.2s\n",
    "√âtape 1000: train 0.9699, val 1.0209, temps 132.1s\n",
    "√âtape 1500: train 0.8488, val 0.9194, temps 195.2s\n",
    "√âtape 2000: train 0.7945, val 0.8735, temps 261.1s\n",
    "√âtape 2500: train 0.7526, val 0.8458, temps 324.2s\n",
    "√âtape 3000: train 0.7040, val 0.8088, temps 389.5s\n",
    "√âtape 3500: train 0.6828, val 0.7919, temps 452.7s\n",
    "√âtape 4000: train 0.6517, val 0.7646, temps 517.5s\n",
    "√âtape 4500: train 0.6294, val 0.7514, temps 580.6s\n",
    "√âtape 5000: train 0.6255, val 0.7525, temps 645.6s\n",
    "√âtape 5500: train 0.6045, val 0.7314, temps 708.7s\n",
    "√âtape 6000: train 0.5819, val 0.7215, temps 773.1s\n",
    "√âtape 6500: train 0.5725, val 0.7142, temps 836.3s\n",
    "√âtape 7000: train 0.5620, val 0.7078, temps 900.9s\n",
    "√âtape 7500: train 0.5445, val 0.6998, temps 964.2s\n",
    "√âtape 8000: train 0.5407, val 0.6941, temps 1029.0s\n",
    "√âtape 8500: train 0.5390, val 0.6958, temps 1092.5s\n",
    "√âtape 9000: train 0.5285, val 0.6848, temps 1156.7s\n",
    "√âtape 9500: train 0.5146, val 0.6801, temps 1220.0s\n",
    "√âtape 10000: train 0.5198, val 0.6785, temps 1284.6s\n",
    "√âtape 10500: train 0.4981, val 0.6715, temps 1348.0s\n",
    "√âtape 11000: train 0.4952, val 0.6720, temps 1412.5s\n",
    "√âtape 11500: train 0.4886, val 0.6643, temps 1475.8s\n",
    "√âtape 12000: train 0.4793, val 0.6595, temps 1540.8s\n",
    "√âtape 12500: train 0.4781, val 0.6618, temps 1604.3s\n",
    "√âtape 13000: train 0.4640, val 0.6522, temps 1669.2s\n",
    "√âtape 13500: train 0.4686, val 0.6523, temps 1732.5s\n",
    "√âtape 14000: train 0.4658, val 0.6482, temps 1797.6s\n",
    "√âtape 14500: train 0.4593, val 0.6496, temps 1860.8s\n",
    "√âtape 14999: train 0.4441, val 0.6414, temps 1924.1s\n",
    "\"\"\"\n",
    "\n",
    "# --- Extraire les donn√©es avec regex ---\n",
    "pattern = r\"√âtape (\\d+): train ([0-9.]+), val ([0-9.]+)\"\n",
    "matches = re.findall(pattern, log_text)\n",
    "\n",
    "steps = [int(m[0]) for m in matches]\n",
    "train_losses = [float(m[1]) for m in matches]\n",
    "val_losses = [float(m[2]) for m in matches]\n",
    "\n",
    "# ---Cr√©er un DataFrame pour plus de clart√© ---\n",
    "df = pd.DataFrame({\n",
    "    \"step\": steps,\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_loss\": val_losses\n",
    "})\n",
    "print(df.head())\n",
    "\n",
    "# ---  Plot ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df[\"step\"], df[\"train_loss\"], label=\"Train\", marker='o', linewidth=2)\n",
    "plt.plot(df[\"step\"], df[\"val_loss\"], label=\"Validation\", marker='s', linewidth=2)\n",
    "plt.title(\"Courbe d'apprentissage (train vs validation)\")\n",
    "plt.xlabel(\"It√©rations\")\n",
    "plt.ylabel(\"Perte moyenne (cross-entropy)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"train_val_loss.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4e8cd-cb53-4cbe-ad5c-083715f8bea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model_with_metrics(model, stoi, itos, dataset, conditions_list, n_samples=100):\n",
    "    \"\"\"√âvalue le mod√®le avec plusieurs m√©triques et cr√©e une table + graphiques.\"\"\"\n",
    "    \n",
    "    results_all = []\n",
    "    seen_smiles = {Chem.MolToSmiles(Chem.MolFromSmiles(decode(x.tolist()[1:-1]))) \n",
    "                   for x, _, _ in dataset if Chem.MolFromSmiles(decode(x.tolist()[1:-1]))}\n",
    "\n",
    "    for cond_label, cond_vec in conditions_list:\n",
    "        cond_tensor = torch.tensor(cond_vec, dtype=torch.float32).unsqueeze(0)\n",
    "        generated = [generate_with_retry(model, cond_tensor, stoi, itos, max_retries=3) for _ in range(n_samples)]\n",
    "        \n",
    "        valid = [s for s in generated if Chem.MolFromSmiles(s)]\n",
    "        unique = set(valid)\n",
    "        novel = [s for s in unique if s not in seen_smiles]\n",
    "        \n",
    "        # Calcul respect conditions\n",
    "        matches = 0\n",
    "        for s in valid:\n",
    "            status, logp, mw, hbd = check_mol_3_props(s)\n",
    "            if (get_category(logp, LOGP_BINS) == cond_vec[0] and\n",
    "                get_category(mw, MW_BINS) == cond_vec[1] and\n",
    "                get_category(hbd, HBD_BINS) == cond_vec[2]):\n",
    "                matches += 1\n",
    "\n",
    "        results_all.append({\n",
    "            \"Condition\": cond_label,\n",
    "            \"Validit√© (%)\": len(valid)/n_samples*100,\n",
    "            \"Unicit√© (%)\": len(unique)/n_samples*100,\n",
    "            \"Nouveaut√© (%)\": len(novel)/n_samples*100,\n",
    "            \"Respect conditions (%)\": matches/n_samples*100\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results_all)\n",
    "    print(\"\\nüìä Tableau r√©capitulatif :\")\n",
    "    print(df.round(2))\n",
    "\n",
    "    # Graphiques\n",
    "    plt.figure(figsize=(8,5))\n",
    "    df.set_index(\"Condition\")[[\"Validit√© (%)\", \"Unicit√© (%)\", \"Nouveaut√© (%)\", \"Respect conditions (%)\"]].plot(kind=\"bar\")\n",
    "    plt.title(\"√âvaluation du mod√®le de g√©n√©ration mol√©culaire\")\n",
    "    plt.ylabel(\"Pourcentage (%)\")\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640606cb-839a-4ccb-8162-7c021eaa71c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditions_to_test = [\n",
    "    (\"[LogP(1), MW(1), HBD(3)]\", [1.0, 1.0, 3.0]),\n",
    "    (\"[LogP(3), MW(3), HBD(0)]\", [3.0, 3.0, 0.0]),\n",
    "    (\"[LogP(2), MW(2), HBD(1)]\", [2.0, 2.0, 1.0]),\n",
    "]\n",
    "\n",
    "df_results = evaluate_model_with_metrics(model, stoi, itos, dataset, conditions_to_test, n_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d71571-dbce-4a30-b6c3-bd3f5d96065b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.82177,
   "end_time": "2025-10-21T19:59:11.934774",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-21T19:59:05.113004",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
