{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61e49c5-6d42-4bec-aa5e-59863e1ee7aa",
   "metadata": {
    "papermill": {
     "duration": 0.006483,
     "end_time": "2025-10-21T19:59:10.706915",
     "exception": false,
     "start_time": "2025-10-21T19:59:10.700432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3526b052-3dfa-449b-840e-31f559559081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T19:59:10.716811Z",
     "iopub.status.busy": "2025-10-21T19:59:10.715860Z",
     "iopub.status.idle": "2025-10-21T19:59:11.507756Z",
     "shell.execute_reply": "2025-10-21T19:59:11.506226Z"
    },
    "papermill": {
     "duration": 0.798713,
     "end_time": "2025-10-21T19:59:11.509763",
     "exception": true,
     "start_time": "2025-10-21T19:59:10.711050",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BackendError",
     "evalue": "POST failed with: {\"errors\":[\"New Datasets cannot be attached in non-interactive sessions. Found no versions attached for Dataset [ouical/data-gen].\"],\"error\":{\"code\":9},\"wasSuccessful\":false}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/3090221043.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Download latest version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ouical/data-gen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Path to dataset files:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Dataset: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/kaggle_cache_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mdataset_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"VersionNumber\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion_from_package_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         result = client.post(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mATTACH_DATASOURCE_REQUEST_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             {\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, request_name, data, timeout)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wasSuccessful\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"POST failed with: {response.text!s}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBackendError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"result\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'result' field missing from response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBackendError\u001b[0m: POST failed with: {\"errors\":[\"New Datasets cannot be attached in non-interactive sessions. Found no versions attached for Dataset [ouical/data-gen].\"],\"error\":{\"code\":9},\"wasSuccessful\":false}"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ouical/data-gen\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb489a86-31d1-47d9-bd75-0dc8cf51dbd3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SCRIPT COMPLET AMÉLIORÉ : TRANSFORMATEUR CONDITIONNEL (Catégoriel) -\n",
    "\"\"\"\n",
    "\n",
    "# --- PARTIE 1 : IMPORTS ET CONFIGURATION ---\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from rdkit import rdBase\n",
    "    rdBase.DisableLog('rdApp.error')\n",
    "except ImportError:\n",
    "    print(\"Erreur : RDKit n'est pas installé.\")\n",
    "    print(\"Veuillez l'installer avec : pip install rdkit\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725477d-8917-4d0f-bd29-2b87b4edb86b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration AMÉLIORÉE ---\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 128\n",
    "MAX_ITERS = 15000  # Augmenté\n",
    "EVAL_INTERVAL = 500\n",
    "LEARNING_RATE = 1e-4  # Réduit\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EVAL_ITERS = 200\n",
    "N_EMBD = 256  # Augmenté\n",
    "N_HEAD = 8    # Augmenté\n",
    "N_LAYER = 8   # Augmenté\n",
    "DROPOUT = 0.1\n",
    "CONDITION_DIM = 3\n",
    "\n",
    "# Fichiers\n",
    "DATA_FILE = 's_100_str_1M_fixed.txt'\n",
    "VOCAB_FILE = 'vocab_dataset.json'\n",
    "DATA_CACHE_FILE = 'data_cache_categorical_improved.pt'\n",
    "\n",
    "# Checkpoints\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, 'cond_gpt_categorical_improved.pth')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration : Périphérique={DEVICE}, Batch={BATCH_SIZE}, Contexte={BLOCK_SIZE}\")\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135268e-0e03-4980-9f1c-369c89ad22c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------ UTILITAIRE LECTURE SMILES ------------------\n",
    "def yield_smiles_from_file(filepath):\n",
    "    \"\"\"Rend chaque SMILES du fichier\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('<') and '><' in line:\n",
    "                for smi in line.strip().strip('<>').split('><'):\n",
    "                    smi = smi.strip()\n",
    "                    if smi:\n",
    "                        yield smi\n",
    "            else:\n",
    "                yield line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a1e4c-a267-45aa-b1cd-ae5a5f3e2751",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 2 : CONSTRUCTION DU VOCABULAIRE AMÉLIORÉE ---\n",
    "\n",
    "print(f\"Construction du vocabulaire à partir de '{DATA_FILE}'...\")\n",
    "\n",
    "PAD_TOKEN = '<pad>'\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "\n",
    "# Construction du vocabulaire avec normalisation\n",
    "char_set = set()\n",
    "n_seen = 0\n",
    "valid_smiles_count = 0\n",
    "\n",
    "for smiles in yield_smiles_from_file(DATA_FILE):\n",
    "    # Pré-filtrage avec RDKit\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        try:\n",
    "            canon_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "            char_set.update(list(canon_smiles))\n",
    "            valid_smiles_count += 1\n",
    "        except:\n",
    "            continue\n",
    "    n_seen += 1\n",
    "\n",
    "# Vocabulaire\n",
    "special_tokens = [PAD_TOKEN, START_TOKEN, END_TOKEN]\n",
    "vocabulary = special_tokens + sorted(set(char_set))\n",
    "\n",
    "# Dictionnaires\n",
    "stoi = { ch:i for i,ch in enumerate(vocabulary) }\n",
    "itos = { i:ch for i,ch in enumerate(vocabulary) }\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Sauvegarde\n",
    "with open(VOCAB_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump({'stoi': stoi, 'itos': itos}, f, indent=2)\n",
    "\n",
    "print(f\"Taille vocabulaire : {vocab_size} | SMILES valides: {valid_smiles_count}/{n_seen}\")\n",
    "\n",
    "# Fonctions encode/decode\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "decode = lambda l: ''.join([itos[i] for i in l if i in itos])\n",
    "\n",
    "# Test\n",
    "print(\"\\n=== TEST VOCABULAIRE ===\")\n",
    "test_smiles = \"CCO\"\n",
    "encoded = encode(test_smiles)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Test: {test_smiles} -> Encoded: {encoded} -> Decoded: {decoded} -> Match: {test_smiles == decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d17f24-ac4b-46b4-bf9a-1b0abb138112",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 3 : PRÉPARATION DES DONNÉES AMÉLIORÉE ---\n",
    "\n",
    "LOGP_BINS = [0.0, 3.0, 5.0]\n",
    "MW_BINS   = [250.0, 480.0, 650.0]\n",
    "HBD_BINS  = [0.0, 1.0, 2.0, 3.0]\n",
    "\n",
    "def get_category(value, bins):\n",
    "    for i, upper_bound in enumerate(bins):\n",
    "        if value <= upper_bound:\n",
    "            return float(i)\n",
    "    return float(len(bins))\n",
    "\n",
    "def load_and_process_data_improved(filepath, stoi, max_len=BLOCK_SIZE, cache_file=DATA_CACHE_FILE):\n",
    "    \"\"\"Version améliorée avec normalisation des SMILES\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Chargement des données depuis le cache '{cache_file}'...\")\n",
    "        data = torch.load(cache_file)\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            print(f\"Données chargées ({len(data)} exemples).\")\n",
    "            return data\n",
    "        else:\n",
    "            print(\"Cache vide/corrompu, re-traitement...\")\n",
    "\n",
    "    print(\"Traitement des données SMILES avec normalisation...\")\n",
    "    data_processed = []\n",
    "    pad_idx = stoi[PAD_TOKEN]\n",
    "    start_idx = stoi[START_TOKEN]\n",
    "    end_idx = stoi[END_TOKEN]\n",
    "\n",
    "    try:\n",
    "        for i, smiles in enumerate(tqdm(yield_smiles_from_file(filepath), desc=\"Traitement\")):\n",
    "            # Vérification RDKit\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Normalisation\n",
    "                canon_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "                \n",
    "                # Vérification longueur et caractères\n",
    "                if len(canon_smiles) > max_len - 2 or not all(c in stoi for c in canon_smiles):\n",
    "                    continue\n",
    "\n",
    "                # Calcul propriétés\n",
    "                logp = Descriptors.MolLogP(mol)\n",
    "                mw   = Descriptors.MolWt(mol)\n",
    "                hbd  = Descriptors.NumHDonors(mol)\n",
    "\n",
    "                logp_cat = get_category(logp, LOGP_BINS)\n",
    "                mw_cat   = get_category(mw, MW_BINS)\n",
    "                hbd_cat  = get_category(hbd, HBD_BINS)\n",
    "\n",
    "                condition_vector = torch.tensor([logp_cat, mw_cat, hbd_cat], dtype=torch.float32)\n",
    "\n",
    "                # Encodage\n",
    "                token_ids = [start_idx] + encode(canon_smiles) + [end_idx]\n",
    "                seq_len = len(token_ids)\n",
    "                \n",
    "                x = torch.full((max_len,), pad_idx, dtype=torch.long)\n",
    "                y = torch.full((max_len,), pad_idx, dtype=torch.long)\n",
    "                x[:seq_len] = torch.tensor(token_ids, dtype=torch.long)\n",
    "                y[:seq_len-1] = torch.tensor(token_ids[1:], dtype=torch.long)\n",
    "\n",
    "                data_processed.append((x, y, condition_vector))\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            if i > 0 and i % 50000 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERREUR: Fichier '{filepath}' introuvable.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"\\nNombre total de molécules valides : {len(data_processed)}\")\n",
    "    \n",
    "    if len(data_processed) == 0:\n",
    "        print(\"⚠️ Aucun exemple valide.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Sauvegarde dans le cache '{cache_file}'...\")\n",
    "    torch.save(data_processed, cache_file)\n",
    "    return data_processed\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42b7f3-ff23-467c-9ad4-f5f589099413",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 4 : ARCHITECTURE DU MODÈLE (INCHANGÉE MAIS AVEC CONFIG AMÉLIORÉE) ---\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    vocab_size: int = vocab_size\n",
    "    n_layer: int = N_LAYER\n",
    "    n_head: int = N_HEAD\n",
    "    n_embd: int = N_EMBD\n",
    "    dropout: float = DROPOUT\n",
    "    condition_dim: int = CONDITION_DIM\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class ConditionalDrugGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "\n",
    "        self.condition_projector = nn.Sequential(\n",
    "            nn.Linear(config.condition_dim, config.n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, conditions=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Séquence trop longue: {T}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "\n",
    "        assert conditions is not None, \"Conditions requises !\"\n",
    "        cond_emb = self.condition_projector(conditions)\n",
    "\n",
    "        x = self.transformer.drop(tok_emb + pos_emb + cond_emb.unsqueeze(1))\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=stoi[PAD_TOKEN]\n",
    "            )\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6093d-29f4-42e6-bd4c-6c1b0bb5e59a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 5 : FONCTIONS UTILITAIRES AMÉLIORÉES ---\n",
    "\n",
    "def save_checkpoint(model, optimizer, iter_num, best_val_loss, config, filepath):\n",
    "    print(f\"Sauvegarde du checkpoint dans {filepath}...\")\n",
    "    torch.save({\n",
    "        'iter_num': iter_num,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': config,\n",
    "    }, filepath)\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"Aucun checkpoint trouvé. Démarrage d'un nouvel entraînement.\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "    print(f\"Chargement du checkpoint depuis {filepath}...\")\n",
    "    checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Reprise à l'itération {iter_num} (meilleure perte val: {best_val_loss:.4f})\")\n",
    "    return iter_num, best_val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader, eval_iters=EVAL_ITERS):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        loader = train_loader if split == 'train' else val_loader\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        loader_iter = iter(loader)\n",
    "        for k in range(eval_iters):\n",
    "            try:\n",
    "                x, y, c = next(loader_iter)\n",
    "            except StopIteration:\n",
    "                loader_iter = iter(loader)\n",
    "                x, y, c = next(loader_iter)\n",
    "\n",
    "            x, y, c = x.to(DEVICE), y.to(DEVICE), c.to(DEVICE)\n",
    "            logits, loss = model(x, y, c)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21db8d-ecf8-4654-9a9f-c08f1676f637",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 6 : GÉNÉRATION AMÉLIORÉE ---\n",
    "\n",
    "def validate_and_repair_smiles(smiles):\n",
    "    \"\"\"Tente de réparer les SMILES invalides\"\"\"\n",
    "    if not smiles or smiles == \"[VIDE]\":\n",
    "        return None\n",
    "        \n",
    "    # Essayer directement\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    \n",
    "    # Tentatives de réparation\n",
    "    try:\n",
    "        # Fermer les parenthèses\n",
    "        open_count = smiles.count('(')\n",
    "        close_count = smiles.count(')')\n",
    "        if open_count > close_count:\n",
    "            smiles += ')' * (open_count - close_count)\n",
    "        \n",
    "        # Fermer les cycles\n",
    "        ring_numbers = set()\n",
    "        for char in smiles:\n",
    "            if char.isdigit():\n",
    "                ring_numbers.add(char)\n",
    "        \n",
    "        for ring_num in ring_numbers:\n",
    "            if smiles.count(ring_num) % 2 != 0:\n",
    "                smiles += ring_num\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_conditional_improved(model, condition_tensor, stoi, itos, max_new_tokens=80, temperature=0.8, top_k=12):\n",
    "    \"\"\"Version améliorée de la génération\"\"\"\n",
    "    model.eval()\n",
    "    start_idx = stoi[START_TOKEN]\n",
    "    end_idx = stoi[END_TOKEN]\n",
    "\n",
    "    idx = torch.tensor([[start_idx]], dtype=torch.long, device=DEVICE)\n",
    "    condition_tensor = condition_tensor.to(DEVICE)\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "        \n",
    "        logits, _ = model(idx_cond, conditions=condition_tensor)\n",
    "        logits = logits[:, -1, :] / max(temperature, 0.1)  # Éviter division par zéro\n",
    "        \n",
    "        # Top-k filtering\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Échantillonnage avec contrainte pour éviter les tokens improbables\n",
    "        if torch.isnan(probs).any() or torch.max(probs) < 0.01:\n",
    "            idx_next = torch.tensor([[stoi['C']]], device=DEVICE)  # Fallback\n",
    "        else:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Arrêt si token de fin\n",
    "        if idx_next.item() == end_idx:\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        # Arrêt précoce si séquence trop longue\n",
    "        if idx.size(1) >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    # Décodage\n",
    "    generated_tokens = idx[0].tolist()\n",
    "    tokens_to_decode = generated_tokens[1:]  # Exclure <start>\n",
    "    \n",
    "    # Trouver le premier <end>\n",
    "    if end_idx in tokens_to_decode:\n",
    "        end_pos = tokens_to_decode.index(end_idx)\n",
    "        tokens_to_decode = tokens_to_decode[:end_pos]\n",
    "    \n",
    "    generated_smiles = decode(tokens_to_decode)\n",
    "    \n",
    "    return generated_smiles if generated_smiles else \"[VIDE]\"\n",
    "\n",
    "def generate_with_retry(model, condition_tensor, stoi, itos, max_retries=3):\n",
    "    \"\"\"Génère avec plusieurs tentatives\"\"\"\n",
    "    best_smiles = None\n",
    "    best_length = 0\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        # Ajuster la température progressivement\n",
    "        temp = 0.7 + attempt * 0.2\n",
    "        top_k = 10 + attempt * 5\n",
    "        \n",
    "        smiles = generate_conditional_improved(\n",
    "            model, condition_tensor, stoi, itos, \n",
    "            temperature=temp,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        # Tenter réparation\n",
    "        repaired = validate_and_repair_smiles(smiles)\n",
    "        if repaired and Chem.MolFromSmiles(repaired):\n",
    "            # Préférer les SMILES de longueur raisonnable\n",
    "            if 10 <= len(repaired) <= 60:\n",
    "                return repaired\n",
    "            elif best_smiles is None or abs(len(repaired) - 35) < abs(best_length - 35):\n",
    "                best_smiles = repaired\n",
    "                best_length = len(repaired)\n",
    "    \n",
    "    return best_smiles if best_smiles else \"[VIDE]\"\n",
    "\n",
    "def check_mol_3_props(smiles):\n",
    "    \"\"\"Vérifie les 3 propriétés réelles\"\"\"\n",
    "    if not smiles or smiles == \"[VIDE]\":\n",
    "        return \"Vide\", 0.0, 0.0, 0\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return \"Invalide\", 0.0, 0.0, 0\n",
    "    try:\n",
    "        logp = Descriptors.MolLogP(mol)\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        hbd = Descriptors.NumHDonors(mol)\n",
    "        return \"Valide\", logp, mw, hbd\n",
    "    except:\n",
    "        return \"Erreur\", 0.0, 0.0, 0\n",
    "\n",
    "def evaluate_generation_quality(generated_smiles, target_conditions):\n",
    "    \"\"\"Évalue la qualité des générations\"\"\"\n",
    "    results = {\n",
    "        'valid': 0,\n",
    "        'matching_conditions': 0,\n",
    "        'unique': 0,\n",
    "        'avg_length': 0\n",
    "    }\n",
    "    \n",
    "    unique_smiles = set()\n",
    "    valid_smiles = []\n",
    "    \n",
    "    for smi in generated_smiles:\n",
    "        status, logp, mw, hbd = check_mol_3_props(smi)\n",
    "        \n",
    "        if status == \"Valide\":\n",
    "            results['valid'] += 1\n",
    "            unique_smiles.add(smi)\n",
    "            valid_smiles.append(smi)\n",
    "            results['avg_length'] += len(smi)\n",
    "            \n",
    "            # Vérifier correspondance conditions\n",
    "            logp_cat = get_category(logp, LOGP_BINS)\n",
    "            mw_cat = get_category(mw, MW_BINS)\n",
    "            hbd_cat = get_category(hbd, HBD_BINS)\n",
    "            \n",
    "            if (logp_cat == target_conditions[0] and \n",
    "                mw_cat == target_conditions[1] and \n",
    "                hbd_cat == target_conditions[2]):\n",
    "                results['matching_conditions'] += 1\n",
    "    \n",
    "    results['unique'] = len(unique_smiles)\n",
    "    if results['valid'] > 0:\n",
    "        results['avg_length'] /= results['valid']\n",
    "    \n",
    "    return results, valid_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1b2f5-4792-44a9-8228-85b22216764b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PARTIE 7 : SCRIPT PRINCIPAL AMÉLIORÉ ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1. Chargement des données amélioré\n",
    "    full_data = load_and_process_data_improved(DATA_FILE, stoi, cache_file=DATA_CACHE_FILE)\n",
    "    if len(full_data) == 0:\n",
    "        print(\"Arrêt : aucun échantillon valide.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Équilibrage des classes\n",
    "    print(\"\\n⚖️ Calcul des poids d'échantillonnage...\")\n",
    "    combo_counts = Counter([tuple(sample[2].tolist()) for sample in full_data])\n",
    "    total_samples = len(full_data)\n",
    "    num_classes = len(combo_counts)\n",
    "\n",
    "    weights = []\n",
    "    for _, _, cond in full_data:\n",
    "        key = tuple(cond.tolist())\n",
    "        weights.append(total_samples / (num_classes * combo_counts[key]))\n",
    "\n",
    "    print(f\"✅ Combinaisons de classes : {num_classes}\")\n",
    "    print(f\"Exemple de poids : {list(weights)[:5]}\")\n",
    "\n",
    "    # 3. Dataset et DataLoaders\n",
    "    dataset = SMILESDataset(full_data)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_indices = train_data.indices\n",
    "    train_weights = [weights[i] for i in train_indices]\n",
    "    sampler = WeightedRandomSampler(train_weights, num_samples=len(train_indices), replacement=True)\n",
    "\n",
    "    pin_mem = True if DEVICE == 'cuda' else False\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=sampler, \n",
    "                             num_workers=0, pin_memory=pin_mem)\n",
    "    val_loader   = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             num_workers=0, pin_memory=pin_mem)\n",
    "\n",
    "    print(\"✅ Échantillonnage équilibré activé.\")\n",
    "\n",
    "    # 4. Modèle avec configuration améliorée\n",
    "    config = GPTConfig(vocab_size=vocab_size)\n",
    "    model = ConditionalDrugGPT(config).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    \n",
    "    param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    print(f\"Nombre de paramètres : {param_count:.2f} M\")\n",
    "\n",
    "    # 5. Checkpoint\n",
    "    start_iter, best_val_loss = load_checkpoint(CHECKPOINT_FILE, model, optimizer)\n",
    "\n",
    "    # 6. Entraînement amélioré\n",
    "    print(f\"🚀 Début de l'entraînement sur {DEVICE}...\")\n",
    "    start_time = time.time()\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    for iter_num in range(start_iter, MAX_ITERS):\n",
    "        try:\n",
    "            xb, yb, cb = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            xb, yb, cb = next(train_iter)\n",
    "\n",
    "        xb, yb, cb = xb.to(DEVICE), yb.to(DEVICE), cb.to(DEVICE)\n",
    "        logits, loss = model(xb, targets=yb, conditions=cb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter_num % 1000 == 0:\n",
    "    results3 = test_generation_complete(\"[LogP(2), MW(2), HBD(1)]\", [2.0, 2.0, 1.0])\n",
    "    \n",
    "    # Résumé final\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 RÉSUMÉ FINAL DES PERFORMANCES\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Validité moyenne: {(results1['valid'] + results2['valid'] + results3['valid']) / 90 * 100:.1f}%\")\n",
    "    print(f\"Correspondance moyenne: {(results1['matching_conditions'] + results2['matching_conditions'] + results3['matching_conditions']) / 90 * 100:.1f}%\")\n",
    "    print(f\"Unicité moyenne: {(results1['unique'] + results2['unique'] + results3['unique']) / 90 * 100:.1f}%\")\n",
    "\n",
    "    print(\"\\n✅ Script terminé avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbede6cc-5fce-4c18-8496-15bb90bc4f81",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---  Copier ici ton texte de log ---\n",
    "log_text = \"\"\"\n",
    "Étape 500: train 1.2334, val 1.2766, temps 67.2s\n",
    "Étape 1000: train 0.9699, val 1.0209, temps 132.1s\n",
    "Étape 1500: train 0.8488, val 0.9194, temps 195.2s\n",
    "Étape 2000: train 0.7945, val 0.8735, temps 261.1s\n",
    "Étape 2500: train 0.7526, val 0.8458, temps 324.2s\n",
    "Étape 3000: train 0.7040, val 0.8088, temps 389.5s\n",
    "Étape 3500: train 0.6828, val 0.7919, temps 452.7s\n",
    "Étape 4000: train 0.6517, val 0.7646, temps 517.5s\n",
    "Étape 4500: train 0.6294, val 0.7514, temps 580.6s\n",
    "Étape 5000: train 0.6255, val 0.7525, temps 645.6s\n",
    "Étape 5500: train 0.6045, val 0.7314, temps 708.7s\n",
    "Étape 6000: train 0.5819, val 0.7215, temps 773.1s\n",
    "Étape 6500: train 0.5725, val 0.7142, temps 836.3s\n",
    "Étape 7000: train 0.5620, val 0.7078, temps 900.9s\n",
    "Étape 7500: train 0.5445, val 0.6998, temps 964.2s\n",
    "Étape 8000: train 0.5407, val 0.6941, temps 1029.0s\n",
    "Étape 8500: train 0.5390, val 0.6958, temps 1092.5s\n",
    "Étape 9000: train 0.5285, val 0.6848, temps 1156.7s\n",
    "Étape 9500: train 0.5146, val 0.6801, temps 1220.0s\n",
    "Étape 10000: train 0.5198, val 0.6785, temps 1284.6s\n",
    "Étape 10500: train 0.4981, val 0.6715, temps 1348.0s\n",
    "Étape 11000: train 0.4952, val 0.6720, temps 1412.5s\n",
    "Étape 11500: train 0.4886, val 0.6643, temps 1475.8s\n",
    "Étape 12000: train 0.4793, val 0.6595, temps 1540.8s\n",
    "Étape 12500: train 0.4781, val 0.6618, temps 1604.3s\n",
    "Étape 13000: train 0.4640, val 0.6522, temps 1669.2s\n",
    "Étape 13500: train 0.4686, val 0.6523, temps 1732.5s\n",
    "Étape 14000: train 0.4658, val 0.6482, temps 1797.6s\n",
    "Étape 14500: train 0.4593, val 0.6496, temps 1860.8s\n",
    "Étape 14999: train 0.4441, val 0.6414, temps 1924.1s\n",
    "\"\"\"\n",
    "\n",
    "# --- Extraire les données avec regex ---\n",
    "pattern = r\"Étape (\\d+): train ([0-9.]+), val ([0-9.]+)\"\n",
    "matches = re.findall(pattern, log_text)\n",
    "\n",
    "steps = [int(m[0]) for m in matches]\n",
    "train_losses = [float(m[1]) for m in matches]\n",
    "val_losses = [float(m[2]) for m in matches]\n",
    "\n",
    "# ---Créer un DataFrame pour plus de clarté ---\n",
    "df = pd.DataFrame({\n",
    "    \"step\": steps,\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_loss\": val_losses\n",
    "})\n",
    "print(df.head())\n",
    "\n",
    "# ---  Plot ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df[\"step\"], df[\"train_loss\"], label=\"Train\", marker='o', linewidth=2)\n",
    "plt.plot(df[\"step\"], df[\"val_loss\"], label=\"Validation\", marker='s', linewidth=2)\n",
    "plt.title(\"Courbe d'apprentissage (train vs validation)\")\n",
    "plt.xlabel(\"Itérations\")\n",
    "plt.ylabel(\"Perte moyenne (cross-entropy)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"train_val_loss.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4e8cd-cb53-4cbe-ad5c-083715f8bea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model_with_metrics(model, stoi, itos, dataset, conditions_list, n_samples=100):\n",
    "    \"\"\"Évalue le modèle avec plusieurs métriques et crée une table + graphiques.\"\"\"\n",
    "    \n",
    "    results_all = []\n",
    "    seen_smiles = {Chem.MolToSmiles(Chem.MolFromSmiles(decode(x.tolist()[1:-1]))) \n",
    "                   for x, _, _ in dataset if Chem.MolFromSmiles(decode(x.tolist()[1:-1]))}\n",
    "\n",
    "    for cond_label, cond_vec in conditions_list:\n",
    "        cond_tensor = torch.tensor(cond_vec, dtype=torch.float32).unsqueeze(0)\n",
    "        generated = [generate_with_retry(model, cond_tensor, stoi, itos, max_retries=3) for _ in range(n_samples)]\n",
    "        \n",
    "        valid = [s for s in generated if Chem.MolFromSmiles(s)]\n",
    "        unique = set(valid)\n",
    "        novel = [s for s in unique if s not in seen_smiles]\n",
    "        \n",
    "        # Calcul respect conditions\n",
    "        matches = 0\n",
    "        for s in valid:\n",
    "            status, logp, mw, hbd = check_mol_3_props(s)\n",
    "            if (get_category(logp, LOGP_BINS) == cond_vec[0] and\n",
    "                get_category(mw, MW_BINS) == cond_vec[1] and\n",
    "                get_category(hbd, HBD_BINS) == cond_vec[2]):\n",
    "                matches += 1\n",
    "\n",
    "        results_all.append({\n",
    "            \"Condition\": cond_label,\n",
    "            \"Validité (%)\": len(valid)/n_samples*100,\n",
    "            \"Unicité (%)\": len(unique)/n_samples*100,\n",
    "            \"Nouveauté (%)\": len(novel)/n_samples*100,\n",
    "            \"Respect conditions (%)\": matches/n_samples*100\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results_all)\n",
    "    print(\"\\n📊 Tableau récapitulatif :\")\n",
    "    print(df.round(2))\n",
    "\n",
    "    # Graphiques\n",
    "    plt.figure(figsize=(8,5))\n",
    "    df.set_index(\"Condition\")[[\"Validité (%)\", \"Unicité (%)\", \"Nouveauté (%)\", \"Respect conditions (%)\"]].plot(kind=\"bar\")\n",
    "    plt.title(\"Évaluation du modèle de génération moléculaire\")\n",
    "    plt.ylabel(\"Pourcentage (%)\")\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640606cb-839a-4ccb-8162-7c021eaa71c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditions_to_test = [\n",
    "    (\"[LogP(1), MW(1), HBD(3)]\", [1.0, 1.0, 3.0]),\n",
    "    (\"[LogP(3), MW(3), HBD(0)]\", [3.0, 3.0, 0.0]),\n",
    "    (\"[LogP(2), MW(2), HBD(1)]\", [2.0, 2.0, 1.0]),\n",
    "]\n",
    "\n",
    "df_results = evaluate_model_with_metrics(model, stoi, itos, dataset, conditions_to_test, n_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d71571-dbce-4a30-b6c3-bd3f5d96065b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.82177,
   "end_time": "2025-10-21T19:59:11.934774",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-21T19:59:05.113004",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
